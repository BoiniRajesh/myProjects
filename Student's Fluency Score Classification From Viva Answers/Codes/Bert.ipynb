{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSvZMVuykigQ"
      },
      "source": [
        "dataset balance check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMIj0mxe_32w",
        "outputId": "9c6a8905-95ec-45d1-fff4-4e616f8ea606"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "463\n",
            "1290\n",
            "1089\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_excel(\"student_embeddings_output.xlsx\")\n",
        "low = data[data[\"Numerical_Label\"] == 0]\n",
        "med = data[data[\"Numerical_Label\"] == 1]\n",
        "high = data[data[\"Numerical_Label\"] == 2]\n",
        "\n",
        "print(len(low))\n",
        "print(len(med))\n",
        "print(len(high))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06yoAQoWAZl_"
      },
      "source": [
        "balancing dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wnbqrobAcnn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "# Drop non-numeric columns (like 'Student' and 'Teacher')\n",
        "X = data.drop(columns=['Student_Text', 'Teacher_Text', 'Fluency_Label'])\n",
        "\n",
        "# Convert column names to strings\n",
        "X.columns = X.columns.astype(str)\n",
        "\n",
        "# Target column\n",
        "y = data['Numerical_Label']\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Create DataFrame from resampled data\n",
        "X_resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "y_resampled_df = pd.DataFrame(y_resampled, columns=['Label'])\n",
        "\n",
        "# Combine into one DataFrame\n",
        "balanced_df = pd.concat([X_resampled_df, y_resampled_df], axis=1)\n",
        "\n",
        "# Save to Excel\n",
        "balanced_df.to_excel(\"final_student_embeddings_upsampled.xlsx\", index=False)\n",
        "print(\"Upsampled dataset saved successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CssrT-CHA_O-"
      },
      "source": [
        "checking balance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE4SOmQ7BC0U",
        "outputId": "3ac96ad9-0cac-4652-c334-e90cbd64502d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1290\n",
            "1290\n",
            "1290\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_excel(\"final_student_embeddings_upsampled.xlsx\")\n",
        "low = data[data[\"Numerical_Label\"] == 0]\n",
        "med = data[data[\"Numerical_Label\"] == 1]\n",
        "high = data[data[\"Numerical_Label\"] == 2]\n",
        "\n",
        "print(len(low))\n",
        "print(len(med))\n",
        "print(len(high))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFVsHhC-BjN6"
      },
      "source": [
        "NCA and Lime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lime"
      ],
      "metadata": {
        "id": "be7T8O9rU_fz",
        "outputId": "17a34e9b-9b34-4521-96b0-f60d52f1738a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/275.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from lime) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from lime) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lime) (1.15.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from lime) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.11/dist-packages (from lime) (1.6.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.11/dist-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (11.2.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2025.3.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=f62693cc6d227d99968728792c656e08ab5757a81bc5505029daaa5c5198aa2a\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/fa/a3/9c2d44c9f3cd77cf4e533b58900b2bf4487f2a17e8ec212a3d\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vk_vKsEiCAw6",
        "outputId": "c4402a3e-54e5-418d-9f39-227e83fba386"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Top 10 original features contributing to NCA Component 1:\n",
            "Feature 21: Weight = 6.5062\n",
            "Feature 614: Weight = 6.4133\n",
            "Feature 622: Weight = 5.7168\n",
            "Feature 247: Weight = 5.4134\n",
            "Feature 241: Weight = 5.3150\n",
            "Feature 296: Weight = 5.1558\n",
            "Feature 747: Weight = -5.1285\n",
            "Feature 499: Weight = 5.1238\n",
            "Feature 9: Weight = 5.0146\n",
            "Feature 258: Weight = 4.9513\n",
            "\n",
            "üîç Top 10 original features contributing to NCA Component 2:\n",
            "Feature 441: Weight = 6.8877\n",
            "Feature 236: Weight = 6.4547\n",
            "Feature 96: Weight = -5.8187\n",
            "Feature 549: Weight = 5.7652\n",
            "Feature 416: Weight = 5.7212\n",
            "Feature 14: Weight = -5.6918\n",
            "Feature 421: Weight = -5.6676\n",
            "Feature 261: Weight = 5.4887\n",
            "Feature 45: Weight = 5.4108\n",
            "Feature 340: Weight = -5.3644\n",
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'C': 0.01, 'solver': 'lbfgs'}\n",
            "Tuning SVM...\n",
            "Best parameters for SVM: {'C': 10, 'kernel': 'rbf'}\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n",
            "No tuning required for Naive Bayes\n",
            "Tuning Decision Tree...\n",
            "Best parameters for Decision Tree: {'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'learning_rate': 1.0, 'n_estimators': 200}\n",
            "Tuning MLP...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for MLP: {'activation': 'tanh', 'hidden_layer_sizes': (100, 50), 'learning_rate': 'constant'}\n",
            "Tuning KNN...\n",
            "Best parameters for KNN: {'n_neighbors': 9, 'weights': 'uniform'}\n",
            "Tuning XGBoost...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:09:18] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}\n",
            "üìå Logistic Regression\n",
            "Training Accuracy: 0.7930\n",
            "Testing Accuracy:  0.8049\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.96      0.89       265\n",
            "           1       0.80      0.68      0.73       260\n",
            "           2       0.79      0.78      0.78       249\n",
            "\n",
            "    accuracy                           0.80       774\n",
            "   macro avg       0.80      0.80      0.80       774\n",
            "weighted avg       0.80      0.80      0.80       774\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå SVM\n",
            "Training Accuracy: 0.8317\n",
            "Testing Accuracy:  0.8295\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.94      0.93       265\n",
            "           1       0.74      0.80      0.77       260\n",
            "           2       0.82      0.75      0.79       249\n",
            "\n",
            "    accuracy                           0.83       774\n",
            "   macro avg       0.83      0.83      0.83       774\n",
            "weighted avg       0.83      0.83      0.83       774\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå Random Forest\n",
            "Training Accuracy: 0.9076\n",
            "Testing Accuracy:  0.8385\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.93      0.93       265\n",
            "           1       0.76      0.80      0.78       260\n",
            "           2       0.83      0.78      0.80       249\n",
            "\n",
            "    accuracy                           0.84       774\n",
            "   macro avg       0.84      0.84      0.84       774\n",
            "weighted avg       0.84      0.84      0.84       774\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå Naive Bayes\n",
            "Training Accuracy: 0.7875\n",
            "Testing Accuracy:  0.7933\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       265\n",
            "           1       0.80      0.66      0.72       260\n",
            "           2       0.74      0.79      0.76       249\n",
            "\n",
            "    accuracy                           0.79       774\n",
            "   macro avg       0.79      0.79      0.79       774\n",
            "weighted avg       0.79      0.79      0.79       774\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå Decision Tree\n",
            "Training Accuracy: 0.8391\n",
            "Testing Accuracy:  0.8152\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.92      0.92       265\n",
            "           1       0.73      0.79      0.76       260\n",
            "           2       0.80      0.73      0.76       249\n",
            "\n",
            "    accuracy                           0.82       774\n",
            "   macro avg       0.82      0.81      0.81       774\n",
            "weighted avg       0.82      0.82      0.82       774\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå AdaBoost\n",
            "Training Accuracy: 0.7807\n",
            "Testing Accuracy:  0.7855\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.87       265\n",
            "           1       0.77      0.67      0.72       260\n",
            "           2       0.74      0.78      0.76       249\n",
            "\n",
            "    accuracy                           0.79       774\n",
            "   macro avg       0.78      0.78      0.78       774\n",
            "weighted avg       0.78      0.79      0.78       774\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå MLP\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.8547\n",
            "Testing Accuracy:  0.8501\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.94      0.94       265\n",
            "           1       0.78      0.82      0.80       260\n",
            "           2       0.84      0.80      0.82       249\n",
            "\n",
            "    accuracy                           0.85       774\n",
            "   macro avg       0.85      0.85      0.85       774\n",
            "weighted avg       0.85      0.85      0.85       774\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå KNN\n",
            "Training Accuracy: 0.8534\n",
            "Testing Accuracy:  0.8540\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.94      0.93       265\n",
            "           1       0.78      0.83      0.81       260\n",
            "           2       0.85      0.79      0.82       249\n",
            "\n",
            "    accuracy                           0.85       774\n",
            "   macro avg       0.85      0.85      0.85       774\n",
            "weighted avg       0.85      0.85      0.85       774\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå XGBoost\n",
            "Training Accuracy: 0.8811\n",
            "Testing Accuracy:  0.8398\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.93      0.93       265\n",
            "           1       0.76      0.80      0.78       260\n",
            "           2       0.83      0.78      0.80       249\n",
            "\n",
            "    accuracy                           0.84       774\n",
            "   macro avg       0.84      0.84      0.84       774\n",
            "weighted avg       0.84      0.84      0.84       774\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå Stacking (All Models)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:09:31] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:09:44] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:10:29] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.8650\n",
            "Testing Accuracy:  0.8553\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.93      0.93       265\n",
            "           1       0.78      0.82      0.80       260\n",
            "           2       0.85      0.81      0.83       249\n",
            "\n",
            "    accuracy                           0.86       774\n",
            "   macro avg       0.86      0.85      0.85       774\n",
            "weighted avg       0.86      0.86      0.86       774\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "LIME Explanations for Test Instance (First Sample, original features):\n",
            "Instance values: [ 2.27361605e-01 -4.91949618e-02  9.42274034e-02 -1.35573804e-01\n",
            " -1.22863680e-01 -1.63560539e-01  1.57175645e-01  6.39255270e-02\n",
            "  7.28624389e-02  2.32553110e-03 -1.12150244e-01  3.02667648e-01\n",
            "  6.47887588e-02  1.19132608e-01 -1.35609612e-01  1.80780012e-02\n",
            "  4.50311273e-01  4.24533635e-02  7.81470761e-02 -1.94882363e-01\n",
            " -6.72378093e-02 -3.21377814e-01 -1.02093697e-01 -2.93038815e-01\n",
            " -1.29985303e-01 -1.56096131e-01  1.25415146e-01 -4.83175553e-02\n",
            "  4.96174693e-02  1.95012525e-01 -5.31670675e-02  5.56749254e-02\n",
            " -6.35362491e-02  7.15797618e-02 -6.32672682e-02 -1.53083622e-01\n",
            " -1.58302695e-01  7.47656226e-02 -8.40972662e-02  3.22855473e-01\n",
            "  1.68660223e-01 -1.66942179e-01  1.90699279e-01  6.81761652e-02\n",
            "  3.40599895e-01 -1.11963218e-02 -1.34216234e-01  2.86069423e-01\n",
            " -2.08150581e-01  3.62062231e-02  7.57789165e-02 -2.65063316e-01\n",
            " -1.72591671e-01  2.71052897e-01 -2.37420738e-01 -1.46334618e-02\n",
            "  2.40484085e-02 -1.63603097e-01  7.93455243e-02 -9.10391733e-02\n",
            " -2.46865287e-01 -1.05388597e-01  1.11255199e-01  1.65720478e-01\n",
            " -9.60611627e-02  1.13320127e-01  1.92837954e-01 -1.43712670e-01\n",
            " -8.55459720e-02  1.86498798e-02  1.63761228e-02 -3.93162668e-01\n",
            " -2.76685983e-01  1.85068116e-01 -6.69379085e-02 -5.84358387e-02\n",
            "  1.33672103e-01 -1.61732659e-02 -9.19382051e-02  1.61548570e-01\n",
            "  1.20624853e-03  1.88963339e-01  4.56158146e-02  1.60978839e-01\n",
            " -7.75131434e-02  1.12428680e-01  2.62305886e-01 -7.68474489e-02\n",
            " -1.85656816e-01  5.08989215e-01  7.21895173e-02  1.22393623e-01\n",
            " -1.74860880e-01 -1.56144112e-01  7.40090609e-02  1.93210334e-01\n",
            "  1.33578137e-01  3.26344579e-01  1.19895358e-02  1.37854680e-01\n",
            "  2.27558181e-01  3.78970861e-01 -8.67848396e-02  1.15823410e-01\n",
            "  2.32488140e-01  8.10979530e-02 -6.40082657e-02 -2.43131399e-01\n",
            " -5.88767268e-02  5.38652718e-01  4.64130491e-01 -3.26846272e-01\n",
            "  2.78437227e-01 -2.13695690e-01  1.05059534e-01 -1.18148535e-01\n",
            "  1.09498203e-01  1.13192283e-01 -2.41949514e-01  1.11999974e-01\n",
            "  1.27990544e-01 -7.02282116e-02 -1.32583454e-01  1.16088942e-01\n",
            "  7.58489519e-02 -1.45603092e-02 -1.23375453e-01  4.91610691e-02\n",
            " -1.09040216e-01  1.88493803e-01 -3.28290164e-01  1.38027996e-01\n",
            " -1.55668855e-01  3.42051871e-03 -1.33880312e-02  1.29791588e-01\n",
            " -2.54256070e-01 -2.00747371e-01 -4.64930505e-01  2.90756673e-02\n",
            " -1.05129823e-01 -1.49021059e-01 -1.81631818e-02 -6.28079548e-02\n",
            "  1.20070368e-01  2.27547944e-01 -1.22691192e-01 -1.22290313e-01\n",
            "  3.34112309e-02  3.22293460e-01  9.44071412e-02  1.40442163e-01\n",
            "  2.23187312e-01  4.59098965e-02 -3.06390114e-02 -1.16211195e-02\n",
            " -1.65538527e-02 -9.98888761e-02  3.27202410e-01 -5.14554530e-02\n",
            "  2.07600862e-01 -3.25563669e-01 -1.21214397e-01 -2.47497171e-01\n",
            "  2.46923208e-01  1.19264781e-01 -5.14231063e-02  3.21760066e-02\n",
            "  2.33811498e-01  1.55382037e-01  9.98291187e-03 -1.02577969e-01\n",
            " -3.04126918e-01 -5.06404489e-02 -1.16450876e-01  2.89855003e-01\n",
            "  8.73064250e-02 -9.72272828e-02 -3.50323925e-03  1.72024637e-01\n",
            "  8.80568549e-02 -1.30936384e-01 -2.43523613e-01 -2.49640524e-01\n",
            " -3.72299179e-02  2.45929640e-02  1.13145627e-01 -3.03199962e-02\n",
            "  1.15196690e-01 -9.35311988e-02  3.13250452e-01  4.84864041e-03\n",
            " -1.29195452e-02  2.75509447e-01 -3.35341066e-01 -3.13478649e-01\n",
            "  1.64089166e-02  6.32881448e-02  1.06218576e-01 -1.62930787e-01\n",
            " -3.36570710e-01 -8.56676698e-02 -2.07046978e-02  1.75463259e-01\n",
            " -7.60690197e-02 -1.77198738e-01  1.94876894e-01  4.32791948e-01\n",
            "  1.23593491e-03 -1.94878459e-01 -2.06705272e-01 -1.79485589e-01\n",
            "  7.86825791e-02 -6.75207973e-02  2.22700328e-01 -2.14656070e-01\n",
            "  1.19059585e-01  1.46414842e-02 -1.21913798e-01  6.92500398e-02\n",
            "  1.77889735e-01 -1.53511122e-01  1.42747462e-01 -5.52427135e-02\n",
            " -9.85661894e-02  4.19587269e-02  1.37736559e-01 -4.29550633e-02\n",
            "  1.33912370e-01 -2.41147682e-01  4.88488451e-02  3.07180434e-01\n",
            " -1.98676018e-04 -2.11758971e-01  2.52483398e-01 -3.81034255e-01\n",
            " -2.51138210e-01  2.75980413e-01 -1.58303201e-01 -1.13767803e-01\n",
            " -3.05890501e-01  1.99142978e-01  2.58695297e-02  2.13059425e-01\n",
            "  2.00425833e-03  5.92877805e-01 -9.44624543e-02  4.96337228e-02\n",
            "  1.46436036e-01 -3.27694491e-02  1.75491929e-01 -3.06479018e-02\n",
            " -6.04326248e-01  6.33104295e-02  2.77517110e-01 -1.44110024e-01\n",
            "  1.13592125e-01 -3.19195837e-01  6.86915144e-02 -2.21384123e-01\n",
            "  3.47284138e-01  1.00731559e-01  1.06681809e-01 -2.27422670e-01\n",
            "  9.05889943e-02 -2.48549446e-01  1.48828238e-01  7.21086636e-02\n",
            "  1.34604082e-01  4.28804308e-02 -1.60156280e-01 -2.68855486e-02\n",
            "  2.18153477e-01  3.49798083e-01 -6.70610927e-03  1.08114131e-01\n",
            "  2.23570690e-01 -7.49733150e-02  3.07345152e-01  4.96190675e-02\n",
            "  9.84501913e-02 -1.36461049e-01  6.32879734e-02  4.90960538e-01\n",
            " -3.82518172e-01 -2.25873403e-02  1.12251699e-01  2.19235629e-01\n",
            "  2.25709170e-01  3.33636925e-02  3.38383839e-02 -1.73836902e-01\n",
            "  7.02322572e-02  1.67663679e-01  5.34343123e-02  1.30021319e-01\n",
            " -2.24154532e-01  2.47071117e-01 -7.87172616e-02  1.36309922e-01\n",
            "  2.06851326e-02  5.74425235e-03 -1.04574658e-01  2.53018141e-01\n",
            " -3.36837441e-01  9.43618864e-02 -1.67261288e-01  6.13210276e-02\n",
            " -2.37520397e-01  5.71087152e-02  5.05551398e-01 -2.30896533e-01\n",
            " -6.64858241e-03 -5.36376089e-02 -2.56083518e-01 -4.52217430e-01\n",
            "  1.74671292e-01  1.08211935e-02 -3.41720320e-02 -1.58743754e-01\n",
            "  4.31379899e-02  2.82125711e-01 -2.07608223e-01  2.47892067e-01\n",
            " -6.81121647e-03  9.13437754e-02  9.25960392e-02 -3.64851296e-01\n",
            " -1.13367133e-01 -1.87056631e-01  1.76833689e-01  1.20887384e-02\n",
            " -6.83358461e-02  4.31838408e-02 -3.27762812e-01  2.27225766e-01\n",
            " -2.44704150e-02  2.85978802e-02  1.47643974e-02  6.09267410e-03\n",
            "  1.11625589e-01  2.63093505e-02  6.44466728e-02 -1.29877664e-02\n",
            "  2.78054684e-01  1.16466060e-01  2.55185097e-01 -1.08604692e-01\n",
            "  2.29494810e-01  8.57652277e-02 -6.10023551e-02 -1.32105872e-01\n",
            " -7.20154166e-01  2.99190432e-01  5.22626638e-01 -1.00263953e-01\n",
            "  1.11406840e-01  1.64514594e-02  1.64202843e-02  2.02283353e-01\n",
            "  7.30789006e-02  9.05740447e-03 -1.86188407e-02  3.52089614e-01\n",
            " -6.40905742e-03  1.31952539e-01 -2.95815133e-02  6.70697447e-03\n",
            " -1.73588246e-01  2.22467571e-01  2.18705162e-01 -4.32887822e-02\n",
            " -2.06494525e-01  4.86047789e-02  1.08966231e-01 -1.69121195e-03\n",
            " -4.84591350e-02 -4.52708453e-04  3.74590717e-02 -1.39950261e-01\n",
            "  2.31213458e-02  4.13215488e-01  9.06558558e-02  6.70084953e-02\n",
            " -6.95568323e-02 -1.96280301e-01 -1.29061043e-01 -1.57292277e-01\n",
            "  1.57077700e-01 -1.30310450e-02  1.24757834e-01 -2.45631427e-01\n",
            "  8.13703984e-02  2.28988081e-01  1.23977050e-01  1.01087116e-01\n",
            "  9.96436179e-02 -4.64465544e-02  4.41315584e-02 -2.09953070e-01\n",
            " -2.26513907e-01  1.09495059e-01 -6.15324490e-02 -5.29815555e-02\n",
            "  2.38349196e-02 -5.26649952e-02  3.56380492e-02  4.38549556e-02\n",
            "  2.76338169e-03  2.70949900e-01 -3.10420394e-01 -1.46152049e-01\n",
            " -1.06897458e-01  3.19779292e-02  2.50765029e-02 -1.86716914e-01\n",
            " -9.93651450e-02  8.80249366e-02  2.96521366e-01 -2.18013048e-01\n",
            " -3.10329702e-02  1.04057997e-01  4.72178340e-01 -4.08425331e-01\n",
            "  8.30721259e-02  3.76008637e-02  1.91270232e-01 -2.92568922e-01\n",
            " -5.72105423e-02  3.17345083e-01 -8.62119794e-02  8.12472925e-02\n",
            " -1.85029045e-01  7.15824142e-02 -2.84823537e-01 -5.53467460e-02\n",
            " -2.36466564e-02 -1.85879678e-01 -1.33068413e-01  1.87750235e-01\n",
            "  8.85007009e-02 -3.05183306e-02 -1.48287779e-02 -2.61344194e-01\n",
            "  1.18869759e-01  4.14013825e-02  1.42278932e-02  1.07654162e-01\n",
            "  7.03643113e-02 -2.11768135e-01 -3.69422495e-01 -2.85399631e-02\n",
            "  1.09177314e-01 -2.00397253e-01  1.97737053e-01  1.14227466e-01\n",
            " -1.04480609e-01 -1.03630006e-01 -7.47513548e-02 -4.28683311e-02\n",
            " -8.13429430e-03  3.60427126e-02 -1.36325806e-01  3.09402227e-01\n",
            "  1.91156253e-01  2.47197419e-01  9.32651758e-02  3.44709039e-01\n",
            " -1.61802933e-01  6.42482862e-02 -2.67482586e-02 -1.53771400e-01\n",
            "  1.41049940e-02  3.02086473e-01  9.74646658e-02  2.02621415e-01\n",
            "  1.42516717e-01  1.56922620e-02 -1.62318185e-01 -1.95450887e-01\n",
            " -2.12182999e-01  3.12874764e-01 -1.55404776e-01  2.88283229e-01\n",
            "  2.05649987e-01  3.35385650e-02 -4.57836017e-02 -1.45894259e-01\n",
            "  3.91307086e-01  1.60337225e-01  1.14521660e-01  4.77415979e-01\n",
            "  7.21537098e-02  1.18326247e-02  1.64034501e-01  7.57204667e-02\n",
            "  6.60597235e-02  4.04757448e-02 -1.35991320e-01 -4.76431921e-02\n",
            "  1.16878331e-01  9.10736769e-02 -2.27786154e-01  5.61515912e-02\n",
            " -1.09449454e-01 -7.32037723e-02 -1.44368589e-01  6.34679422e-02\n",
            "  1.84602827e-01  6.64184541e-02  3.29223871e-02 -1.30822137e-01\n",
            " -1.02337651e-01  1.61859132e-02 -2.88160797e-02 -1.91475093e-01\n",
            " -1.15010619e-01 -8.52297544e-02  1.59811229e-01  1.26914993e-01\n",
            "  5.13521135e-02  9.00444388e-02 -8.77580792e-03  2.22376049e-01\n",
            "  1.38003588e-01 -7.48850331e-02  1.84456676e-01  1.24857530e-01\n",
            " -3.15347850e-01 -2.09261820e-01  1.00219131e-01 -8.30634609e-02\n",
            " -1.06446169e-01 -2.54656076e-01  2.77240843e-01  3.23637664e-01\n",
            " -6.55652434e-02  3.41483578e-02  1.33367479e-01  2.28452664e-02\n",
            "  1.59111053e-01  2.79392004e-01  1.10914245e-01  3.52327049e-01\n",
            " -7.23548532e-02  4.46865410e-01  3.06550991e-02  2.11926430e-01\n",
            " -1.76278166e-02 -2.84002330e-02  2.03980878e-01  2.59789973e-01\n",
            " -5.70198894e-01  6.21965647e-01  3.00159037e-01  9.35467333e-02\n",
            " -3.78839135e-01 -9.39595127e+00 -6.83439821e-02 -6.83625937e-02\n",
            "  3.91532481e-01 -5.03650168e-03  1.42185930e-02  3.20386052e-01\n",
            "  1.42431129e-02 -1.21670356e-03 -7.00875223e-02  6.36961088e-02\n",
            " -1.37309581e-01  1.53914869e-01  1.49268415e-02 -3.67581159e-01\n",
            " -1.29645109e-01 -2.34149262e-01 -2.54977494e-01 -9.08241645e-02\n",
            "  5.66987216e-01 -2.71393694e-02  2.08815321e-01 -2.64522638e-02\n",
            " -4.05378640e-03 -4.43339236e-02  6.75130635e-02 -9.94323492e-02\n",
            "  3.38282771e-02  4.28593546e-01 -2.86812067e-01 -1.30405962e-01\n",
            " -9.70298946e-02 -1.56360239e-01 -4.31941822e-02 -2.40955651e-01\n",
            " -2.53365636e-01  1.50318250e-01  1.95565179e-01  2.07917262e-02\n",
            " -1.45957202e-01 -4.76303846e-02 -7.42997006e-02  2.43022889e-01\n",
            " -1.25709819e-02  1.20767161e-01 -1.18366428e-01  1.45357162e-01\n",
            " -1.04022287e-01  5.60280532e-02 -1.81301504e-01  1.55274138e-01\n",
            "  4.86676656e-02 -3.52664083e-01 -3.19756866e-02 -9.88722891e-02\n",
            "  1.96502730e-01  7.94841908e-05  1.59263909e-01  1.40450612e-01\n",
            " -1.48854703e-01  1.79936215e-02  2.45511103e-02  2.13943407e-01\n",
            "  4.48778383e-02 -7.64350444e-02  7.45632499e-02 -6.71826862e-03\n",
            " -1.87933698e-01  3.53063852e-01  3.40902865e-01 -2.08097678e-02\n",
            " -2.53566563e-01 -5.59692793e-02  8.70147496e-02  1.02574201e-02\n",
            "  2.89827287e-02  3.19306344e-01 -5.98585606e-02  1.88381210e-01\n",
            "  6.51749432e-01 -1.68265164e-01  8.68920535e-02  3.49843740e-01\n",
            "  9.36081931e-02  7.09563419e-02 -1.56746611e-01 -3.29986691e-01\n",
            " -2.81868100e-01 -3.20881218e-01  2.97250122e-01 -3.54966372e-02\n",
            " -7.55976215e-02 -7.69078881e-02 -1.27565801e-01 -2.95824647e-01\n",
            "  6.35224730e-02 -3.64527851e-02 -1.89970925e-01 -1.89016536e-01\n",
            "  1.75569579e-01  3.25716101e-02  3.60681921e-01  7.86776841e-02\n",
            " -1.21928632e-01 -8.09813961e-02 -2.80298926e-02  2.24234194e-01\n",
            "  1.07704908e-01 -1.22929931e-01 -1.02130435e-01  5.18830866e-02\n",
            " -1.90956146e-02  9.74161550e-02  3.94249409e-02  9.52417105e-02\n",
            "  2.68548667e-01  1.60135940e-01 -1.36712387e-01  4.24111709e-02\n",
            " -2.78344415e-02  3.19242656e-01 -5.49466871e-02  3.57164174e-01\n",
            " -3.73285823e-03 -1.69995889e-01  4.07701135e-02 -2.88150668e-01\n",
            "  2.63457209e-01 -9.50264186e-02  5.55427819e-02  2.59883255e-01\n",
            "  1.59534872e-01 -2.57045269e-01  1.40223913e-02 -2.77968407e-01\n",
            " -2.62886226e-01  8.48318115e-02 -4.26761687e-01 -4.90086704e-01\n",
            " -5.75319715e-02 -1.02982119e-01 -1.18394867e-01 -9.57655907e-03\n",
            "  1.59393281e-01  4.01358157e-02  3.59307565e-02 -3.88577521e-01\n",
            " -6.39891773e-02 -2.58570611e-02  3.68097126e-02  1.14911020e-01\n",
            " -8.94026235e-02  1.25054896e-01 -1.13360681e-01 -2.80809328e-02\n",
            " -3.22770774e-01 -5.88389039e-02 -1.16232574e-01 -4.06091154e-01\n",
            " -2.68081665e-01  1.51986107e-01 -1.35416746e-01  5.89514002e-02\n",
            " -8.25364292e-02 -2.35565439e-01 -1.10591404e-01  1.63502708e-01\n",
            " -4.79063988e-01 -8.38198587e-02  1.07032485e-01 -1.60410926e-01\n",
            " -6.65080026e-02  4.14880179e-03 -1.05656214e-01  2.11867616e-01\n",
            "  2.14977145e-01 -9.70643386e-02 -3.32177460e-01  1.27787486e-01\n",
            " -1.13899997e-02  1.40515164e-01 -1.03264372e-03  5.60829639e-02\n",
            " -6.09262705e-01 -4.62847017e-03  1.28085732e-01 -4.01888490e-02\n",
            " -7.59918243e-02 -1.64865404e-01 -1.00203501e-02  9.30781662e-02\n",
            " -5.61813973e-02 -1.05062775e-01  1.68700591e-01  2.42137372e-01\n",
            "  3.27844441e-01 -9.17208195e-02  4.97476496e-02 -1.53264284e-01\n",
            "  1.39417917e-01  2.92746156e-01  9.39615965e-02 -1.54207468e-01\n",
            "  1.31660908e-01  2.57077180e-02 -9.17704552e-02  1.83278382e-01\n",
            " -2.30396152e-01  5.28163314e-02  1.06094852e-01  1.02874428e-01]\n",
            "Predicted class for each model:\n",
            "Logistic Regression: 1\n",
            "\n",
            "Explaining predictions for Logistic Regression...\n",
            "------------------------------------------------------------\n",
            "SVM: 2\n",
            "\n",
            "Explaining predictions for SVM...\n",
            "------------------------------------------------------------\n",
            "Random Forest: 1\n",
            "\n",
            "Explaining predictions for Random Forest...\n",
            "------------------------------------------------------------\n",
            "Naive Bayes: 2\n",
            "\n",
            "Explaining predictions for Naive Bayes...\n",
            "------------------------------------------------------------\n",
            "Decision Tree: 2\n",
            "\n",
            "Explaining predictions for Decision Tree...\n",
            "------------------------------------------------------------\n",
            "AdaBoost: 2\n",
            "\n",
            "Explaining predictions for AdaBoost...\n",
            "------------------------------------------------------------\n",
            "MLP: 2\n",
            "\n",
            "Explaining predictions for MLP...\n",
            "------------------------------------------------------------\n",
            "KNN: 2\n",
            "\n",
            "Explaining predictions for KNN...\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:16:10] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost: 2\n",
            "\n",
            "Explaining predictions for XGBoost...\n",
            "------------------------------------------------------------\n",
            "Stacking (All Models): Skipped LIME explanation (complex pipeline)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import NeighborhoodComponentsAnalysis\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Optional: XGBoost\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    xgb_available = True\n",
        "except ImportError:\n",
        "    xgb_available = False\n",
        "\n",
        "# Load updated dataset\n",
        "df = pd.read_excel(\"final_student_embeddings_upsampled.xlsx\")\n",
        "\n",
        "# Update this column name if needed\n",
        "target_column = \"Numerical_Label\"\n",
        "\n",
        "# Features and Labels\n",
        "X = df.iloc[:, :768].values\n",
        "y = df[target_column]\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# NCA\n",
        "nca = NeighborhoodComponentsAnalysis(n_components=2, random_state=42)\n",
        "X_nca = nca.fit_transform(X, y_encoded)\n",
        "\n",
        "# Analyze top contributing original features to NCA components\n",
        "nca_components = nca.components_  # Shape: (2, 768)\n",
        "top_k = 10  # Top N contributing features to show\n",
        "\n",
        "for i in range(2):\n",
        "    component = nca_components[i]\n",
        "    top_indices = np.argsort(np.abs(component))[::-1][:top_k]\n",
        "    print(f\"\\nüîç Top {top_k} original features contributing to NCA Component {i + 1}:\")\n",
        "    for idx in top_indices:\n",
        "        print(f\"Feature {idx}: Weight = {component[idx]:.4f}\")\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_nca)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grids for hyperparameter tuning\n",
        "param_grids = {\n",
        "    \"Logistic Regression\": {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'solver': ['lbfgs', 'liblinear']\n",
        "    },\n",
        "    \"SVM\": {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['linear', 'rbf']\n",
        "    },\n",
        "    \"Random Forest\": {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [None, 5, 10],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 5]\n",
        "    },\n",
        "    \"Naive Bayes\": {},\n",
        "    \"Decision Tree\": {\n",
        "        'max_depth': [None, 5, 10],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 5]\n",
        "    },\n",
        "    \"AdaBoost\": {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 1.0]\n",
        "    },\n",
        "    \"MLP\": {\n",
        "        'hidden_layer_sizes': [(50,), (100,), (100, 50)],\n",
        "        'activation': ['relu', 'tanh'],\n",
        "        'learning_rate': ['constant', 'adaptive']\n",
        "    },\n",
        "    \"KNN\": {\n",
        "        'n_neighbors': list(range(1, 21)),\n",
        "        'weights': ['uniform', 'distance']\n",
        "    }\n",
        "}\n",
        "\n",
        "if xgb_available:\n",
        "    param_grids[\"XGBoost\"] = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'learning_rate': [0.01, 0.1, 0.3]\n",
        "    }\n",
        "\n",
        "# Base models\n",
        "base_models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
        "    \"SVM\": SVC(probability=True, random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
        "    \"MLP\": MLPClassifier(max_iter=500, random_state=42),\n",
        "    \"KNN\": KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "if xgb_available:\n",
        "    base_models[\"XGBoost\"] = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "\n",
        "# Hyperparameter tuning\n",
        "best_models = {}\n",
        "for name, model in base_models.items():\n",
        "    if param_grids[name]:\n",
        "        print(f\"Tuning {name}...\")\n",
        "        grid_search = GridSearchCV(\n",
        "            estimator=model,\n",
        "            param_grid=param_grids[name],\n",
        "            cv=5,\n",
        "            scoring='accuracy',\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        best_models[name] = grid_search.best_estimator_\n",
        "        print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "        best_models[name] = model\n",
        "        print(f\"No tuning required for {name}\")\n",
        "\n",
        "# Stacking classifier\n",
        "stacking_estimators = [(name.lower().replace(\" \", \"_\"), model) for name, model in best_models.items()]\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=stacking_estimators,\n",
        "    final_estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Add stacking\n",
        "models = best_models.copy()\n",
        "models[\"Stacking (All Models)\"] = stacking_clf\n",
        "\n",
        "# Evaluate\n",
        "for name, model in models.items():\n",
        "    print(f\"üìå {name}\")\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    train_acc = accuracy_score(y_train, model.predict(X_train))\n",
        "    test_acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Training Accuracy: {train_acc:.4f}\")\n",
        "    print(f\"Testing Accuracy:  {test_acc:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    target_names = [str(label) for label in label_encoder.classes_]\n",
        "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import matplotlib.pyplot as plt  # <-- Add this import\n",
        "\n",
        "# Prepare the test set in original feature space\n",
        "_, X_test_original, _, y_test_original = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42\n",
        ")\n",
        "instance_original = X_test_original[0]\n",
        "\n",
        "# Set up LIME explainer for original features\n",
        "explainer = LimeTabularExplainer(\n",
        "    training_data=X,\n",
        "    feature_names=[f\"emb_{i}\" for i in range(X.shape[1])],\n",
        "    class_names=[str(label) for label in label_encoder.classes_],\n",
        "    mode='classification',\n",
        "    random_state=42,\n",
        "    discretize_continuous=False\n",
        ")\n",
        "\n",
        "print(\"\\nLIME Explanations for Test Instance (First Sample, original features):\")\n",
        "print(f\"Instance values: {instance_original}\")\n",
        "print(\"Predicted class for each model:\")\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Skip stacking for LIME unless you want advanced handling\n",
        "    if name == \"Stacking (All Models)\":\n",
        "        print(f\"{name}: Skipped LIME explanation (complex pipeline)\")\n",
        "        continue\n",
        "\n",
        "    # Build pipeline: StandardScaler -> NCA -> Model (for LIME only)\n",
        "    lime_pipeline = make_pipeline(\n",
        "        StandardScaler(),\n",
        "        NeighborhoodComponentsAnalysis(n_components=2, random_state=42),\n",
        "        model\n",
        "    )\n",
        "    # Fit pipeline on ALL original data (NCA is supervised)\n",
        "    lime_pipeline.fit(X, y_encoded)\n",
        "\n",
        "    pred = lime_pipeline.predict([instance_original])[0]\n",
        "    print(f\"{name}: {label_encoder.inverse_transform([pred])[0]}\")\n",
        "\n",
        "    print(f\"\\nExplaining predictions for {name}...\")\n",
        "    try:\n",
        "        explanation = explainer.explain_instance(\n",
        "            data_row=instance_original,\n",
        "            predict_fn=lime_pipeline.predict_proba,\n",
        "            num_features=10,  # Show top 10 original features\n",
        "            num_samples=5000\n",
        "        )\n",
        "        # Save as HTML\n",
        "        explanation.save_to_file(f\"lime_explanation_{name.replace(' ', '_')}_origfeatures.html\")\n",
        "        # Save as image\n",
        "        fig = explanation.as_pyplot_figure()\n",
        "        plt.tight_layout()\n",
        "        fig.savefig(f\"lime_explanation_{name.replace(' ', '_')}_origfeatures.png\")\n",
        "        plt.close(fig)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to generate explanation for {name}: {str(e)}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTqydjveDBxH"
      },
      "source": [
        "using smot dataset and NCA is not used\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1czMILnDFwO",
        "outputId": "17a80369-e148-4efd-cf60-0a68976e8fa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìå Logistic Regression\n",
            "Training Accuracy: 0.8385\n",
            "Testing Accuracy:  0.5956\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.78      0.72       265\n",
            "           1       0.53      0.43      0.48       260\n",
            "           2       0.56      0.57      0.57       249\n",
            "\n",
            "    accuracy                           0.60       774\n",
            "   macro avg       0.59      0.59      0.59       774\n",
            "weighted avg       0.59      0.60      0.59       774\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå SVM\n",
            "Training Accuracy: 0.9109\n",
            "Testing Accuracy:  0.6382\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.88      0.78       265\n",
            "           1       0.59      0.45      0.51       260\n",
            "           2       0.60      0.57      0.59       249\n",
            "\n",
            "    accuracy                           0.64       774\n",
            "   macro avg       0.63      0.64      0.63       774\n",
            "weighted avg       0.63      0.64      0.63       774\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå Random Forest\n",
            "Training Accuracy: 0.8475\n",
            "Testing Accuracy:  0.5698\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.79      0.72       265\n",
            "           1       0.47      0.34      0.39       260\n",
            "           2       0.54      0.58      0.56       249\n",
            "\n",
            "    accuracy                           0.57       774\n",
            "   macro avg       0.55      0.57      0.56       774\n",
            "weighted avg       0.56      0.57      0.56       774\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå Naive Bayes\n",
            "Training Accuracy: 0.4835\n",
            "Testing Accuracy:  0.4587\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.70      0.58       265\n",
            "           1       0.42      0.34      0.38       260\n",
            "           2       0.44      0.32      0.37       249\n",
            "\n",
            "    accuracy                           0.46       774\n",
            "   macro avg       0.45      0.46      0.44       774\n",
            "weighted avg       0.45      0.46      0.44       774\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå Decision Tree\n",
            "Training Accuracy: 0.5662\n",
            "Testing Accuracy:  0.4496\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.60      0.56       265\n",
            "           1       0.39      0.48      0.43       260\n",
            "           2       0.43      0.26      0.32       249\n",
            "\n",
            "    accuracy                           0.45       774\n",
            "   macro avg       0.45      0.45      0.44       774\n",
            "weighted avg       0.45      0.45      0.44       774\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå AdaBoost\n",
            "Training Accuracy: 0.5342\n",
            "Testing Accuracy:  0.4806\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.54      0.56       265\n",
            "           1       0.42      0.38      0.40       260\n",
            "           2       0.45      0.52      0.48       249\n",
            "\n",
            "    accuracy                           0.48       774\n",
            "   macro avg       0.48      0.48      0.48       774\n",
            "weighted avg       0.48      0.48      0.48       774\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå MLP\n",
            "Training Accuracy: 1.0000\n",
            "Testing Accuracy:  0.6693\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.89      0.84       265\n",
            "           1       0.57      0.46      0.51       260\n",
            "           2       0.62      0.65      0.63       249\n",
            "\n",
            "    accuracy                           0.67       774\n",
            "   macro avg       0.66      0.67      0.66       774\n",
            "weighted avg       0.66      0.67      0.66       774\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå KNN\n",
            "Training Accuracy: 1.0000\n",
            "Testing Accuracy:  0.6111\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.99      0.76       265\n",
            "           1       0.60      0.17      0.27       260\n",
            "           2       0.60      0.67      0.63       249\n",
            "\n",
            "    accuracy                           0.61       774\n",
            "   macro avg       0.61      0.61      0.55       774\n",
            "weighted avg       0.61      0.61      0.55       774\n",
            "\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:36:51] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìå XGBoost\n",
            "Training Accuracy: 1.0000\n",
            "Testing Accuracy:  0.6537\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.88      0.83       265\n",
            "           1       0.57      0.47      0.51       260\n",
            "           2       0.57      0.61      0.59       249\n",
            "\n",
            "    accuracy                           0.65       774\n",
            "   macro avg       0.64      0.65      0.65       774\n",
            "weighted avg       0.65      0.65      0.65       774\n",
            "\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:41:56] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:52:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:53:12] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:54:05] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:55:01] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:55:55] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìå Stacking (All Models)\n",
            "Training Accuracy: 1.0000\n",
            "Testing Accuracy:  0.6835\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.90      0.88       265\n",
            "           1       0.57      0.49      0.53       260\n",
            "           2       0.60      0.65      0.62       249\n",
            "\n",
            "    accuracy                           0.68       774\n",
            "   macro avg       0.68      0.68      0.68       774\n",
            "weighted avg       0.68      0.68      0.68       774\n",
            "\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Optional: XGBoost\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    xgb_available = True\n",
        "except ImportError:\n",
        "    xgb_available = False\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_excel(\"final_student_embeddings_upsampled.xlsx\")\n",
        "\n",
        "# Features and Labels\n",
        "X = df.iloc[:, :768].values\n",
        "y = df[\"Numerical_Label\"]  # No label encoding\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tune KNN\n",
        "knn_param_grid = {'n_neighbors': list(range(1, 21))}\n",
        "knn_grid = GridSearchCV(KNeighborsClassifier(), knn_param_grid, cv=5)\n",
        "knn_grid.fit(X_train, y_train)\n",
        "best_knn = knn_grid.best_estimator_\n",
        "\n",
        "# Base models\n",
        "base_models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
        "    \"SVM\": SVC(kernel='linear', probability=True, random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=6, min_samples_split=10, min_samples_leaf=5, random_state=42),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(max_depth=5, min_samples_split=10, min_samples_leaf=5, random_state=42),\n",
        "    \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42),\n",
        "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', max_iter=500, random_state=42),\n",
        "    \"KNN\": best_knn\n",
        "}\n",
        "\n",
        "if xgb_available:\n",
        "    base_models[\"XGBoost\"] = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "\n",
        "# Stacking Classifier using all models\n",
        "stacking_estimators = [(name.lower().replace(\" \", \"_\"), model) for name, model in base_models.items()]\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=stacking_estimators,\n",
        "    final_estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Add stacking model\n",
        "models = base_models.copy()\n",
        "models[\"Stacking (All Models)\"] = stacking_clf\n",
        "\n",
        "# Train & Evaluate\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    train_acc = accuracy_score(y_train, model.predict(X_train))\n",
        "    test_acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"üìå {name}\")\n",
        "    print(f\"Training Accuracy: {train_acc:.4f}\")\n",
        "    print(f\"Testing Accuracy:  {test_acc:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poIC8q0fDtbx"
      },
      "source": [
        "normal dataset without using NCA and SMOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIfqH9JWD4_K",
        "outputId": "e5be0627-d794-485b-a6cc-280e06fc38e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìå Logistic Regression\n",
            "Training Accuracy: 0.8447\n",
            "Testing Accuracy:  0.4112\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.26      0.26        95\n",
            "           1       0.46      0.47      0.46       258\n",
            "           2       0.42      0.41      0.42       216\n",
            "\n",
            "    accuracy                           0.41       569\n",
            "   macro avg       0.38      0.38      0.38       569\n",
            "weighted avg       0.41      0.41      0.41       569\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå SVM\n",
            "Training Accuracy: 0.9041\n",
            "Testing Accuracy:  0.4165\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.38      0.31        95\n",
            "           1       0.47      0.43      0.45       258\n",
            "           2       0.46      0.41      0.43       216\n",
            "\n",
            "    accuracy                           0.42       569\n",
            "   macro avg       0.40      0.41      0.40       569\n",
            "weighted avg       0.43      0.42      0.42       569\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå Random Forest\n",
            "Training Accuracy: 0.7510\n",
            "Testing Accuracy:  0.4657\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        95\n",
            "           1       0.48      0.78      0.59       258\n",
            "           2       0.44      0.30      0.35       216\n",
            "\n",
            "    accuracy                           0.47       569\n",
            "   macro avg       0.30      0.36      0.31       569\n",
            "weighted avg       0.38      0.47      0.40       569\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå Naive Bayes\n",
            "Training Accuracy: 0.4298\n",
            "Testing Accuracy:  0.4112\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.64      0.37        95\n",
            "           1       0.51      0.39      0.44       258\n",
            "           2       0.52      0.33      0.41       216\n",
            "\n",
            "    accuracy                           0.41       569\n",
            "   macro avg       0.43      0.46      0.41       569\n",
            "weighted avg       0.47      0.41      0.42       569\n",
            "\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìå Decision Tree\n",
            "Training Accuracy: 0.5974\n",
            "Testing Accuracy:  0.4376\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.12      0.18        95\n",
            "           1       0.45      0.67      0.54       258\n",
            "           2       0.41      0.31      0.35       216\n",
            "\n",
            "    accuracy                           0.44       569\n",
            "   macro avg       0.41      0.36      0.36       569\n",
            "weighted avg       0.42      0.44      0.41       569\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå AdaBoost\n",
            "Training Accuracy: 0.5381\n",
            "Testing Accuracy:  0.4552\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.14      0.21        95\n",
            "           1       0.46      0.61      0.52       258\n",
            "           2       0.45      0.41      0.43       216\n",
            "\n",
            "    accuracy                           0.46       569\n",
            "   macro avg       0.46      0.39      0.39       569\n",
            "weighted avg       0.46      0.46      0.44       569\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå MLP\n",
            "Training Accuracy: 0.9996\n",
            "Testing Accuracy:  0.4552\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.28      0.30        95\n",
            "           1       0.49      0.49      0.49       258\n",
            "           2       0.47      0.49      0.48       216\n",
            "\n",
            "    accuracy                           0.46       569\n",
            "   macro avg       0.42      0.42      0.42       569\n",
            "weighted avg       0.45      0.46      0.45       569\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå KNN\n",
            "Training Accuracy: 0.5552\n",
            "Testing Accuracy:  0.4622\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.06      0.11        95\n",
            "           1       0.47      0.65      0.55       258\n",
            "           2       0.45      0.42      0.43       216\n",
            "\n",
            "    accuracy                           0.46       569\n",
            "   macro avg       0.43      0.38      0.36       569\n",
            "weighted avg       0.45      0.46      0.43       569\n",
            "\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [09:59:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìå XGBoost\n",
            "Training Accuracy: 1.0000\n",
            "Testing Accuracy:  0.4534\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.11      0.16        95\n",
            "           1       0.48      0.59      0.53       258\n",
            "           2       0.44      0.44      0.44       216\n",
            "\n",
            "    accuracy                           0.45       569\n",
            "   macro avg       0.41      0.38      0.38       569\n",
            "weighted avg       0.43      0.45      0.43       569\n",
            "\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:03:22] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:11:00] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:11:52] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:12:49] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:13:40] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:14:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìå Stacking (All Models)\n",
            "Training Accuracy: 0.9912\n",
            "Testing Accuracy:  0.4780\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.14      0.19        95\n",
            "           1       0.48      0.66      0.56       258\n",
            "           2       0.51      0.41      0.45       216\n",
            "\n",
            "    accuracy                           0.48       569\n",
            "   macro avg       0.44      0.40      0.40       569\n",
            "weighted avg       0.46      0.48      0.46       569\n",
            "\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Optional: XGBoost\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    xgb_available = True\n",
        "except ImportError:\n",
        "    xgb_available = False\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_excel(\"student_embeddings_output.xlsx\")\n",
        "\n",
        "# Features and Labels\n",
        "X = df.iloc[:, :768].values\n",
        "y = df[\"Numerical_Label\"]  # No label encoding\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tune KNN\n",
        "knn_param_grid = {'n_neighbors': list(range(1, 21))}\n",
        "knn_grid = GridSearchCV(KNeighborsClassifier(), knn_param_grid, cv=5)\n",
        "knn_grid.fit(X_train, y_train)\n",
        "best_knn = knn_grid.best_estimator_\n",
        "\n",
        "# Base models\n",
        "base_models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
        "    \"SVM\": SVC(kernel='linear', probability=True, random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=6, min_samples_split=10, min_samples_leaf=5, random_state=42),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(max_depth=5, min_samples_split=10, min_samples_leaf=5, random_state=42),\n",
        "    \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42),\n",
        "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', max_iter=500, random_state=42),\n",
        "    \"KNN\": best_knn\n",
        "}\n",
        "\n",
        "if xgb_available:\n",
        "    base_models[\"XGBoost\"] = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "\n",
        "# Stacking Classifier using all models\n",
        "stacking_estimators = [(name.lower().replace(\" \", \"_\"), model) for name, model in base_models.items()]\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=stacking_estimators,\n",
        "    final_estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Add stacking model\n",
        "models = base_models.copy()\n",
        "models[\"Stacking (All Models)\"] = stacking_clf\n",
        "\n",
        "# Train & Evaluate\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    train_acc = accuracy_score(y_train, model.predict(X_train))\n",
        "    test_acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"üìå {name}\")\n",
        "    print(f\"Training Accuracy: {train_acc:.4f}\")\n",
        "    print(f\"Testing Accuracy:  {test_acc:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"-\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}