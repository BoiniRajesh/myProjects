{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zY7qHX0963Ke",
        "outputId": "11090fcf-f6f6-4d30-bb50-90ba2ef214b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "452\n",
            "1131\n",
            "1728\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"embedded_dataset_electra.csv\")\n",
        "low = data[data[\"label\"] == 1]\n",
        "med = data[data[\"label\"] == 2]\n",
        "high = data[data[\"label\"] == 3]\n",
        "\n",
        "print(len(low))\n",
        "print(len(med))\n",
        "print(len(high))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "# Drop non-numeric columns (like 'Student' and 'Teacher')\n",
        "data=data.dropna(subset=['label'] + data.columns[:768].tolist())\n",
        "X = data\n",
        "\n",
        "# Convert column names to strings\n",
        "X.columns = X.columns.astype(str)\n",
        "\n",
        "# Target column\n",
        "y = data['label']\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Create DataFrame from resampled data\n",
        "X_resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "y_resampled_df = pd.DataFrame(y_resampled, columns=['Label'])\n",
        "\n",
        "# Combine into one DataFrame\n",
        "balanced_df = pd.concat([X_resampled_df, y_resampled_df], axis=1)\n",
        "\n",
        "# Save to Excel\n",
        "balanced_df.to_excel(\"final_student_embeddings_upsampled.xlsx\", index=False)\n",
        "print(\"Upsampled dataset saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kNJZDaa8KUX",
        "outputId": "e9050eef-9939-4bf9-ff4c-859e833537d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upsampled dataset saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_excel(\"final_student_embeddings_upsampled.xlsx\")\n",
        "low = data[data[\"label\"] == 1]\n",
        "med = data[data[\"label\"] == 2]\n",
        "high = data[data[\"label\"] == 3]\n",
        "\n",
        "print(len(low))\n",
        "print(len(med))\n",
        "print(len(high))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx1K383E8rK-",
        "outputId": "48086aec-1153-4fc1-ad97-cfca7ac65a41"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1728\n",
            "1728\n",
            "1728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A594mWe_9Gfm",
        "outputId": "f9981c0b-3072-4c70-d3d9-68dce35ff8c1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/275.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m204.8/275.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from lime) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from lime) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lime) (1.15.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from lime) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.11/dist-packages (from lime) (1.6.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.11/dist-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (11.2.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2025.3.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=41c759c8f69f95109b000f3825352ca699cd3dbb54dbacbc4bee33654f0d27d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/fa/a3/9c2d44c9f3cd77cf4e533b58900b2bf4487f2a17e8ec212a3d\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NeighborhoodComponentsAnalysis\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Optional: XGBoost\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    xgb_available = True\n",
        "except ImportError:\n",
        "    xgb_available = False\n",
        "\n",
        "# Load updated dataset\n",
        "df = pd.read_excel(\"final_student_embeddings_upsampled.xlsx\")\n",
        "\n",
        "# Update this column name if needed\n",
        "target_column = \"label\"\n",
        "\n",
        "# Features and Labels\n",
        "X = df.iloc[:, :768].values\n",
        "y = df[target_column]\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# NCA\n",
        "nca = NeighborhoodComponentsAnalysis(n_components=2, random_state=42)\n",
        "X_nca = nca.fit_transform(X, y_encoded)\n",
        "\n",
        "# Analyze top contributing original features to NCA components\n",
        "nca_components = nca.components_  # Shape: (2, 768)\n",
        "top_k = 10  # Top N contributing features to show\n",
        "\n",
        "for i in range(2):\n",
        "    component = nca_components[i]\n",
        "    top_indices = np.argsort(np.abs(component))[::-1][:top_k]\n",
        "    print(f\"\\nüîç Top {top_k} original features contributing to NCA Component {i + 1}:\")\n",
        "    for idx in top_indices:\n",
        "        print(f\"Feature {idx}: Weight = {component[idx]:.4f}\")\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_nca)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grids for hyperparameter tuning\n",
        "param_grids = {\n",
        "    \"Logistic Regression\": {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'solver': ['lbfgs', 'liblinear']\n",
        "    },\n",
        "    \"SVM\": {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['linear', 'rbf']\n",
        "    },\n",
        "    \"Random Forest\": {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [None, 5, 10],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 5]\n",
        "    },\n",
        "    \"Naive Bayes\": {},\n",
        "    \"Decision Tree\": {\n",
        "        'max_depth': [None, 5, 10],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 5]\n",
        "    },\n",
        "    \"AdaBoost\": {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 1.0]\n",
        "    },\n",
        "    \"MLP\": {\n",
        "        'hidden_layer_sizes': [(50,), (100,), (100, 50)],\n",
        "        'activation': ['relu', 'tanh'],\n",
        "        'learning_rate': ['constant', 'adaptive']\n",
        "    },\n",
        "    \"KNN\": {\n",
        "        'n_neighbors': list(range(1, 21)),\n",
        "        'weights': ['uniform', 'distance']\n",
        "    }\n",
        "}\n",
        "\n",
        "if xgb_available:\n",
        "    param_grids[\"XGBoost\"] = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'learning_rate': [0.01, 0.1, 0.3]\n",
        "    }\n",
        "\n",
        "# Base models\n",
        "base_models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
        "    \"SVM\": SVC(probability=True, random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
        "    \"MLP\": MLPClassifier(max_iter=500, random_state=42),\n",
        "    \"KNN\": KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "if xgb_available:\n",
        "    base_models[\"XGBoost\"] = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "\n",
        "# Hyperparameter tuning\n",
        "best_models = {}\n",
        "for name, model in base_models.items():\n",
        "    if param_grids[name]:\n",
        "        print(f\"Tuning {name}...\")\n",
        "        grid_search = GridSearchCV(\n",
        "            estimator=model,\n",
        "            param_grid=param_grids[name],\n",
        "            cv=5,\n",
        "            scoring='accuracy',\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        best_models[name] = grid_search.best_estimator_\n",
        "        print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "        best_models[name] = model\n",
        "        print(f\"No tuning required for {name}\")\n",
        "\n",
        "# Stacking classifier\n",
        "stacking_estimators = [(name.lower().replace(\" \", \"_\"), model) for name, model in best_models.items()]\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=stacking_estimators,\n",
        "    final_estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Add stacking\n",
        "models = best_models.copy()\n",
        "models[\"Stacking (All Models)\"] = stacking_clf\n",
        "\n",
        "# Evaluate\n",
        "for name, model in models.items():\n",
        "    print(f\" {name}\")\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    train_acc = accuracy_score(y_train, model.predict(X_train))\n",
        "    test_acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Training Accuracy: {train_acc:.4f}\")\n",
        "    print(f\"Testing Accuracy:  {test_acc:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    target_names = [str(label) for label in label_encoder.classes_]\n",
        "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import matplotlib.pyplot as plt  # <-- Add this import\n",
        "\n",
        "# Prepare the test set in original feature space\n",
        "_, X_test_original, _, y_test_original = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42\n",
        ")\n",
        "instance_original = X_test_original[0]\n",
        "\n",
        "# Set up LIME explainer for original features\n",
        "explainer = LimeTabularExplainer(\n",
        "    training_data=X,\n",
        "    feature_names=[f\"emb_{i}\" for i in range(X.shape[1])],\n",
        "    class_names=[str(label) for label in label_encoder.classes_],\n",
        "    mode='classification',\n",
        "    random_state=42,\n",
        "    discretize_continuous=False\n",
        ")\n",
        "\n",
        "print(\"\\nLIME Explanations for Test Instance (First Sample, original features):\")\n",
        "print(f\"Instance values: {instance_original}\")\n",
        "print(\"Predicted class for each model:\")\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Skip stacking for LIME unless you want advanced handling\n",
        "    if name == \"Stacking (All Models)\":\n",
        "        print(f\"{name}: Skipped LIME explanation (complex pipeline)\")\n",
        "        continue\n",
        "\n",
        "    # Build pipeline: StandardScaler -> NCA -> Model (for LIME only)\n",
        "    lime_pipeline = make_pipeline(\n",
        "        StandardScaler(),\n",
        "        NeighborhoodComponentsAnalysis(n_components=2, random_state=42),\n",
        "        model\n",
        "    )\n",
        "    # Fit pipeline on ALL original data (NCA is supervised)\n",
        "    lime_pipeline.fit(X, y_encoded)\n",
        "\n",
        "    pred = lime_pipeline.predict([instance_original])[0]\n",
        "    print(f\"{name}: {label_encoder.inverse_transform([pred])[0]}\")\n",
        "\n",
        "    print(f\"\\nExplaining predictions for {name}...\")\n",
        "    try:\n",
        "        explanation = explainer.explain_instance(\n",
        "            data_row=instance_original,\n",
        "            predict_fn=lime_pipeline.predict_proba,\n",
        "            num_features=10,  # Show top 10 original features\n",
        "            num_samples=5000\n",
        "        )\n",
        "        # Save as HTML\n",
        "        explanation.save_to_file(f\"lime_explanation_{name.replace(' ', '_')}_origfeatures.html\")\n",
        "        # Save as image\n",
        "        fig = explanation.as_pyplot_figure()\n",
        "        plt.tight_layout()\n",
        "        fig.savefig(f\"lime_explanation_{name.replace(' ', '_')}_origfeatures.png\")\n",
        "        plt.close(fig)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to generate explanation for {name}: {str(e)}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQA_k0ll9I04",
        "outputId": "b865d77d-c721-4203-92fc-b21f87a64428"
      },
      "execution_count": 7,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Top 10 original features contributing to NCA Component 1:\n",
            "Feature 241: Weight = -10.0830\n",
            "Feature 746: Weight = 8.0443\n",
            "Feature 331: Weight = 7.7800\n",
            "Feature 270: Weight = 7.5209\n",
            "Feature 682: Weight = -7.3500\n",
            "Feature 530: Weight = -5.0457\n",
            "Feature 183: Weight = -4.7885\n",
            "Feature 461: Weight = 4.6059\n",
            "Feature 469: Weight = 4.5972\n",
            "Feature 101: Weight = 4.3144\n",
            "\n",
            "üîç Top 10 original features contributing to NCA Component 2:\n",
            "Feature 270: Weight = -14.6004\n",
            "Feature 720: Weight = 13.5697\n",
            "Feature 309: Weight = 6.8455\n",
            "Feature 341: Weight = 5.6401\n",
            "Feature 408: Weight = -5.2257\n",
            "Feature 340: Weight = -4.8985\n",
            "Feature 688: Weight = 4.5241\n",
            "Feature 331: Weight = -4.3955\n",
            "Feature 631: Weight = 4.1551\n",
            "Feature 143: Weight = -4.1353\n",
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'C': 0.01, 'solver': 'lbfgs'}\n",
            "Tuning SVM...\n",
            "Best parameters for SVM: {'C': 10, 'kernel': 'rbf'}\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "No tuning required for Naive Bayes\n",
            "Tuning Decision Tree...\n",
            "Best parameters for Decision Tree: {'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'learning_rate': 1.0, 'n_estimators': 200}\n",
            "Tuning MLP...\n",
            "Best parameters for MLP: {'activation': 'tanh', 'hidden_layer_sizes': (100, 50), 'learning_rate': 'constant'}\n",
            "Tuning KNN...\n",
            "Best parameters for KNN: {'n_neighbors': 20, 'weights': 'uniform'}\n",
            "Tuning XGBoost...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [20:24:31] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
            " Logistic Regression\n",
            "Training Accuracy: 0.7982\n",
            "Testing Accuracy:  0.7994\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.81      0.98      0.89       331\n",
            "           2       0.78      0.76      0.77       344\n",
            "           3       0.80      0.68      0.73       362\n",
            "\n",
            "    accuracy                           0.80      1037\n",
            "   macro avg       0.80      0.80      0.80      1037\n",
            "weighted avg       0.80      0.80      0.79      1037\n",
            "\n",
            "------------------------------------------------------------\n",
            " SVM\n",
            "Training Accuracy: 0.8286\n",
            "Testing Accuracy:  0.8293\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.88      0.97      0.93       331\n",
            "           2       0.81      0.76      0.79       344\n",
            "           3       0.79      0.77      0.78       362\n",
            "\n",
            "    accuracy                           0.83      1037\n",
            "   macro avg       0.83      0.83      0.83      1037\n",
            "weighted avg       0.83      0.83      0.83      1037\n",
            "\n",
            "------------------------------------------------------------\n",
            " Random Forest\n",
            "Training Accuracy: 0.8322\n",
            "Testing Accuracy:  0.8303\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.88      0.97      0.92       331\n",
            "           2       0.81      0.77      0.79       344\n",
            "           3       0.80      0.76      0.78       362\n",
            "\n",
            "    accuracy                           0.83      1037\n",
            "   macro avg       0.83      0.83      0.83      1037\n",
            "weighted avg       0.83      0.83      0.83      1037\n",
            "\n",
            "------------------------------------------------------------\n",
            " Naive Bayes\n",
            "Training Accuracy: 0.8037\n",
            "Testing Accuracy:  0.8120\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.86      0.97      0.91       331\n",
            "           2       0.76      0.81      0.79       344\n",
            "           3       0.81      0.67      0.73       362\n",
            "\n",
            "    accuracy                           0.81      1037\n",
            "   macro avg       0.81      0.82      0.81      1037\n",
            "weighted avg       0.81      0.81      0.81      1037\n",
            "\n",
            "------------------------------------------------------------\n",
            " Decision Tree\n",
            "Training Accuracy: 0.8293\n",
            "Testing Accuracy:  0.8216\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.88      0.96      0.92       331\n",
            "           2       0.76      0.81      0.78       344\n",
            "           3       0.82      0.71      0.76       362\n",
            "\n",
            "    accuracy                           0.82      1037\n",
            "   macro avg       0.82      0.83      0.82      1037\n",
            "weighted avg       0.82      0.82      0.82      1037\n",
            "\n",
            "------------------------------------------------------------\n",
            " AdaBoost\n",
            "Training Accuracy: 0.7953\n",
            "Testing Accuracy:  0.7965\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.83      0.97      0.90       331\n",
            "           2       0.77      0.74      0.75       344\n",
            "           3       0.78      0.69      0.73       362\n",
            "\n",
            "    accuracy                           0.80      1037\n",
            "   macro avg       0.79      0.80      0.79      1037\n",
            "weighted avg       0.79      0.80      0.79      1037\n",
            "\n",
            "------------------------------------------------------------\n",
            " MLP\n",
            "Training Accuracy: 0.8377\n",
            "Testing Accuracy:  0.8235\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.88      0.96      0.92       331\n",
            "           2       0.80      0.77      0.78       344\n",
            "           3       0.79      0.75      0.77       362\n",
            "\n",
            "    accuracy                           0.82      1037\n",
            "   macro avg       0.82      0.83      0.82      1037\n",
            "weighted avg       0.82      0.82      0.82      1037\n",
            "\n",
            "------------------------------------------------------------\n",
            " KNN\n",
            "Training Accuracy: 0.8370\n",
            "Testing Accuracy:  0.8206\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.87      0.96      0.92       331\n",
            "           2       0.79      0.78      0.78       344\n",
            "           3       0.80      0.73      0.76       362\n",
            "\n",
            "    accuracy                           0.82      1037\n",
            "   macro avg       0.82      0.82      0.82      1037\n",
            "weighted avg       0.82      0.82      0.82      1037\n",
            "\n",
            "------------------------------------------------------------\n",
            " XGBoost\n",
            "Training Accuracy: 0.8329\n",
            "Testing Accuracy:  0.8264\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.88      0.96      0.92       331\n",
            "           2       0.80      0.78      0.79       344\n",
            "           3       0.80      0.75      0.77       362\n",
            "\n",
            "    accuracy                           0.83      1037\n",
            "   macro avg       0.83      0.83      0.83      1037\n",
            "weighted avg       0.82      0.83      0.82      1037\n",
            "\n",
            "------------------------------------------------------------\n",
            " Stacking (All Models)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [20:25:12] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [20:25:50] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [20:27:54] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [20:27:55] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.8367\n",
            "Testing Accuracy:  0.8293\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.89      0.96      0.93       331\n",
            "           2       0.81      0.77      0.79       344\n",
            "           3       0.79      0.76      0.77       362\n",
            "\n",
            "    accuracy                           0.83      1037\n",
            "   macro avg       0.83      0.83      0.83      1037\n",
            "weighted avg       0.83      0.83      0.83      1037\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "LIME Explanations for Test Instance (First Sample, original features):\n",
            "Instance values: [ 2.12890029e-01 -1.34479682e-01 -1.32764606e-01 -2.02796557e-01\n",
            "  3.39168891e-01  9.58515781e-01  7.26972669e-01 -2.66439228e-01\n",
            " -7.94369113e-02  1.91280595e-01  5.00898907e-01  2.97939238e-01\n",
            "  6.35831203e-01 -9.05370749e-02  1.79786143e-01 -1.03639507e-01\n",
            " -8.07236202e-02  5.79284901e-01 -2.47611227e-01 -2.53925574e-01\n",
            " -2.52160855e-01 -4.44903599e-01 -7.00799172e-01 -3.93962820e-01\n",
            "  7.50466137e-01 -3.22978953e-01  2.70157824e-01 -6.36592768e-01\n",
            " -1.72062493e-01 -1.27086375e-01  2.68602156e-01 -1.64536404e-01\n",
            "  6.03577656e-01  5.18410353e-01  7.32082329e-01 -2.83936027e-01\n",
            "  9.87076776e-02  1.06742752e+00 -4.00734912e-01 -5.89207621e-01\n",
            "  3.13562350e-01 -3.00079718e-01 -3.00152629e-01 -4.53055888e-01\n",
            "  2.88485341e-01 -1.76529297e-01 -4.92264903e-02 -1.16053354e-01\n",
            " -6.02365935e-01  2.51077653e-01 -8.45088516e-02  3.23156892e-02\n",
            "  5.92608804e-01  5.94507673e-01  2.64173115e-01 -8.11949480e-01\n",
            " -3.09632157e-01  3.56125388e-01  4.50381250e-01  1.79909506e-01\n",
            "  3.32150597e-01 -3.39054795e-01 -2.26035283e-01 -5.13690249e-01\n",
            "  7.98000014e-04 -2.72796351e-01  3.13213041e-01 -1.04262466e-01\n",
            "  7.02061422e-02  7.04784790e-01 -3.06714244e-01 -5.12986779e-02\n",
            " -1.06121618e-02  4.55384560e-01  4.31824872e-01 -2.20837244e-02\n",
            "  4.89600748e-02  3.00228012e-01 -1.68419871e-01 -1.53802887e-01\n",
            " -3.04653037e-02 -3.00172384e-01  3.65947048e-02  3.03070179e-01\n",
            " -2.06685175e-01  6.23340321e-01  6.24483042e-01 -4.59116200e-01\n",
            "  3.83948113e-02 -4.43618934e-01  2.38493296e-01 -1.80774779e-01\n",
            " -1.65034139e-02  2.71085525e-01  8.69211199e-02  5.19588241e-01\n",
            " -7.95931855e-01  1.84688709e-01 -7.52899404e-01 -3.18766048e-01\n",
            " -2.89097646e-01 -3.06195018e-02 -4.11126159e-01 -5.56353299e-01\n",
            " -3.15537186e-01 -1.72388338e-01 -3.67806396e-01  5.50680981e-01\n",
            " -4.94334958e-01  6.35294471e-01 -8.52027702e-02 -1.10978653e-02\n",
            "  3.81243768e-02 -2.61564506e-01 -6.82205201e-01 -1.59870275e-01\n",
            " -5.28029832e-01  1.61289092e-01  1.58653289e-01 -1.05896206e-01\n",
            " -1.07926553e-01  2.41035473e-02  4.77220328e-01 -1.51444238e-01\n",
            "  4.06844182e-01  3.95633391e-01  2.42425717e-01 -3.23644093e-01\n",
            "  3.18794371e-01 -5.22418806e-01 -2.12892442e-01 -5.40968125e-01\n",
            " -5.02172914e-01 -7.11593310e-01  2.03112184e-01 -5.66901669e-01\n",
            "  5.98149638e-02  3.99799706e-01  1.91160426e-01 -5.79913335e-02\n",
            " -3.20104274e-01 -1.96729552e-01 -5.05047568e-01  2.61272058e-01\n",
            "  5.25601371e-01  1.41588491e-01  6.41660257e-01  1.88815822e-01\n",
            " -1.58386803e-02 -1.57285869e-01 -3.57807541e-01  7.23686916e-01\n",
            " -2.58830408e-02  4.47433974e-01 -3.24122946e-01 -1.25588518e-01\n",
            " -3.97876628e-02 -4.47443102e-01  4.65300180e-01  4.39405248e-01\n",
            " -1.75950476e-01  1.95738020e-01  3.89572341e-01 -9.16794559e-02\n",
            " -9.33805338e-02 -8.76317837e-01 -3.15813011e-01 -4.13455896e-01\n",
            " -1.92213571e-01 -3.04188194e-03  7.29593057e-01 -3.31184890e-01\n",
            " -3.21665274e-01  3.59275984e-01 -2.84014035e-02  1.61863977e-01\n",
            "  4.99958807e-02  6.22095646e-01 -2.39173702e-01  6.23934200e-02\n",
            " -7.25096432e-01  1.06554665e-01  1.19244829e-01 -3.86492053e-01\n",
            " -9.94032730e-01 -6.43780186e-01 -4.08132784e-01  9.66572199e-03\n",
            " -2.43936683e-01  1.07472320e-01  7.72285961e-02  1.78061332e-01\n",
            "  3.94616783e-01  4.10715628e-01 -1.54843605e-01  1.54941494e-01\n",
            " -2.67572059e-02  3.71778240e-01 -1.11201562e-01  2.54936663e-01\n",
            " -2.06449126e-01  4.35711018e-03  3.12067670e-01  3.46170745e-01\n",
            " -7.39474404e-02 -7.13523404e-01 -7.92628228e-01 -1.07672570e-01\n",
            "  7.55060144e-02  1.36854725e-01  4.16683636e-01  3.41799665e-02\n",
            "  1.95443624e-01  4.18354634e-01  5.13561120e-01 -1.92432792e-01\n",
            " -3.18930163e-01  2.69448894e-01 -1.67338647e-01 -9.05010251e-02\n",
            "  6.14823406e-01  1.99296369e-01  3.69944035e-01  1.73503395e-01\n",
            "  5.56806309e-01  1.89591855e-01 -3.80183647e-02 -2.10077853e-01\n",
            "  2.34044641e-01  1.95660390e-01  9.97950000e-01  5.38836063e-01\n",
            "  1.55334484e-01  6.36364022e-01  7.81118583e-02  7.37142851e-01\n",
            "  4.62953386e-01  5.08577859e-01 -3.30208543e-01  2.33518819e-01\n",
            " -4.54074969e-01 -1.18391220e+00  3.30666250e-01  1.69505739e-01\n",
            " -7.04390903e-02 -5.03695232e-01  9.54901798e-01  3.49745396e-01\n",
            " -3.90914336e-01  6.49073984e-02 -5.36727588e-01  4.14960687e-01\n",
            " -8.25630644e-02 -2.02297899e-01  5.05458547e-01  4.61481652e-02\n",
            "  3.68996163e-01  7.09173571e-02 -2.36482416e-01 -1.67895960e-01\n",
            "  5.02106596e-01  3.79943242e-02  3.38158563e-01  1.81337437e-01\n",
            "  3.02674274e-01 -3.80013759e-01  3.34531919e-01 -3.02525267e-01\n",
            " -3.08707546e-01  1.72172314e-01 -1.46892967e+00  4.99229528e-01\n",
            "  2.64295282e-02 -3.27058882e-01 -6.77989824e-01 -1.00682420e-01\n",
            " -4.15898318e-01  1.30183145e-01  1.32527995e-01 -1.32206301e-01\n",
            "  1.22878333e-01 -5.57183853e-01  2.81641918e-01  2.94965378e-01\n",
            " -1.99647666e-01  2.98082074e-01  1.60201713e-01 -2.17985915e-01\n",
            " -4.95561294e-01  6.90342311e-01 -2.05788565e-01 -1.74277879e-01\n",
            "  3.93630117e-01  1.95189954e-01  2.20773900e-01 -7.40996491e-01\n",
            "  2.33688559e-01  1.53293880e-01 -3.84139699e-01 -1.13629777e-01\n",
            "  3.09430793e-01  2.85172373e-01  1.90763733e-01 -1.85046519e-01\n",
            " -1.79781470e-02 -1.28826434e-01  2.52966492e-01 -8.96936736e-02\n",
            "  3.10678965e-01 -3.24632431e-01 -1.95351595e-01 -3.82729617e-01\n",
            "  8.28822050e-02 -1.36863701e-01 -4.52619362e-01  1.36231748e-01\n",
            "  2.15171006e-01 -3.41418134e-02 -1.63355705e-02 -1.70511055e-01\n",
            "  1.21758589e-01 -9.52271081e-02 -4.56783849e-01  3.09566314e-01\n",
            "  4.21438062e-01  3.95547381e-01  1.87075611e-01 -1.96045461e-02\n",
            "  1.32963572e-01 -2.45164785e-01  3.35871725e-01 -1.44345878e+00\n",
            " -2.98102104e-01 -7.32028874e-01  9.59323616e-02 -1.38450355e-01\n",
            "  2.48376533e-01  9.00300837e-02  1.38542805e-01  4.02768050e-01\n",
            " -1.92781099e-01  1.12187578e-02 -5.63004085e-01  4.24601867e-01\n",
            " -5.11958063e-01  2.10492443e-01 -7.36467737e-02 -4.13699541e-01\n",
            " -6.95183932e-01  5.98727143e-01  3.20200021e-01  1.51028237e-01\n",
            "  1.41964660e-01  1.91957935e-01 -1.08425422e-01 -6.17590977e-01\n",
            " -6.38803749e-01 -2.38064206e-03  5.98146800e-01  4.84117953e-01\n",
            "  6.67833092e-02 -3.76232360e-02  3.82125891e-01  3.52096273e-01\n",
            " -2.73039603e-01 -2.37972378e-01  3.18725339e-01  4.32511730e-01\n",
            " -2.78197060e-01 -1.80032485e-01 -2.13498780e-01 -5.53718670e-01\n",
            " -4.75524347e-01 -5.43745224e-01 -4.26251376e-01  5.18919204e-01\n",
            " -4.13016517e-01  3.78357313e-01  3.73995573e-01 -2.94529496e-01\n",
            " -5.16255263e-01  6.62886728e-01 -7.15150387e-03  3.24403920e-02\n",
            "  1.24605091e-01  5.66752055e-02  4.34915006e-01 -9.58837039e-02\n",
            "  4.20222303e-01  6.10267673e-01 -3.63358117e-01  3.68925320e-01\n",
            "  5.85216812e-01 -1.32049137e-01  4.97134555e-02 -3.75962646e-01\n",
            "  1.91190508e-02  6.02787113e-01  2.02062532e-02 -2.40146028e-01\n",
            " -4.22247584e-02  3.05974398e-01  7.98437620e-02 -5.69448606e-01\n",
            " -3.61135173e-01 -3.43322224e-02 -4.93028880e-01  2.30675737e-01\n",
            " -7.23608270e-02  3.93629851e-01  3.70128761e-01  1.89110829e-01\n",
            " -3.74509168e-01 -5.68313225e-01  4.41844225e-01 -3.44749306e-01\n",
            " -3.18552859e-01 -8.83838986e-01 -2.72231719e-01 -5.61870220e-01\n",
            " -1.12326892e-01 -2.93422207e-01 -1.51251934e-01 -3.82895903e-01\n",
            " -3.60517989e-01 -2.04397288e-01  2.65177306e-01  1.28510863e-01\n",
            " -8.76536844e-01  2.09473981e-01 -6.91181073e-01  4.26842060e-02\n",
            "  3.05608591e-02 -5.74582367e-02 -6.20247859e-01  2.70985370e-02\n",
            " -1.70194384e-01 -1.01548299e-01  3.99233564e-01 -4.08036450e-01\n",
            " -8.76106566e-02 -8.19279844e-01  2.49903412e-01 -1.00260522e-01\n",
            "  1.10270411e-01 -1.52884912e-01  5.59833148e-01 -3.28956139e-01\n",
            "  2.00990253e-01 -7.50629470e-01  4.13138864e-01  3.56182240e-01\n",
            " -2.51318684e-02  2.64133391e-01 -1.31428148e-01  3.87567770e-01\n",
            "  5.19193738e-01 -7.75128297e-02  6.86481786e-01 -3.92173898e-01\n",
            " -2.33589696e-02 -8.49251070e-01  5.98146009e-02 -2.61846762e-01\n",
            " -7.18446390e-02 -1.71688411e-01  1.14924907e-01  2.70315069e-01\n",
            " -2.53844543e-01  5.26124135e-02 -8.12589124e-01 -1.45144182e-01\n",
            " -5.47055700e-01 -1.35882073e-01 -8.55127818e-01  1.42000393e-01\n",
            " -2.62542148e-01  8.66089671e-01 -2.66711701e-01 -5.72949023e-01\n",
            "  2.16832268e-01 -2.38610907e-01 -2.87593584e-01 -1.74929695e-01\n",
            "  5.77151776e-01 -3.81737386e-01  8.35561412e-02  6.90396926e-01\n",
            "  8.42268792e-01 -7.85164304e-01  1.37634491e-01  3.92802846e-01\n",
            " -5.68742357e-01 -8.87529913e-02  6.35520031e-02 -1.70502636e-01\n",
            " -1.56509925e-01  4.89730917e-01  6.68383528e-01 -7.61705453e-02\n",
            " -2.41915052e-01 -8.63386838e-01 -6.33240682e-01  4.02191574e-01\n",
            " -1.44863188e-01 -1.28737308e-01  2.98236525e-01  1.19354763e-01\n",
            " -9.14488998e-03 -1.18397098e-01  2.18432779e-01  9.62060077e-01\n",
            " -3.51441797e-01  1.07627084e-01  1.74116153e-01  2.49765973e-01\n",
            "  3.19624720e-01 -4.09127315e-01  4.44478494e-01  1.95259313e-01\n",
            " -2.75610160e-01 -1.15323595e+00 -4.35368167e-01  2.95173310e-01\n",
            " -4.17909360e-01 -2.06486190e-01 -4.14445550e-01  5.68766838e-01\n",
            "  1.51604808e-01  2.13515168e-01 -5.26356558e-02  1.66736772e-01\n",
            "  1.15536599e-01 -1.70558317e-01 -1.28407127e-01  4.52818055e-01\n",
            "  1.38250259e-01 -3.22319002e-01 -3.32856769e-02 -1.87053432e-01\n",
            "  3.68043022e-01 -7.17071549e-01 -5.99723051e-01  4.00400264e-01\n",
            " -1.40156727e-01  4.87250263e-01  2.22404588e-01 -3.22973100e-01\n",
            "  5.67017758e-01  6.11363518e-01 -2.76940009e-02 -6.58162150e-02\n",
            "  3.57981369e-02 -1.95689640e-01  4.47077053e-01 -4.48206883e-01\n",
            "  1.98129881e-01 -1.90564269e-01  1.24020400e+00 -3.53218082e-01\n",
            " -3.38823527e-01 -1.70944002e-01 -1.65375830e-01  3.11200991e-01\n",
            " -7.68346748e-01  5.60615799e-01  3.14908984e-01  2.70970306e-01\n",
            "  4.80376285e-03 -1.26341516e-01  3.46023297e-01 -5.27068899e-01\n",
            "  7.70151286e-01 -3.49660016e-01 -7.20646595e-02  6.01344078e-02\n",
            " -8.72975352e-02 -6.45341238e-01 -3.68600555e-01  1.79563344e+00\n",
            "  9.11006134e-02  1.92285152e-02 -3.40436927e-01  2.64452804e-01\n",
            "  4.52927100e-02  3.74295273e-03  5.54995834e-01 -1.56409822e-01\n",
            " -9.40184293e-02  1.12228949e-01  3.86871416e-01  2.96149679e-01\n",
            "  5.48373954e-01  4.62976701e-01 -3.66033232e-01  4.98016738e-02\n",
            "  2.33779064e-01 -2.12490501e-01  3.64654037e-01  2.88292148e-01\n",
            " -6.32865777e-01 -2.76309109e-02 -4.29803685e-01  5.37157570e-01\n",
            " -5.72990606e-01  8.02911189e-02 -1.02710949e+00 -2.18549162e-01\n",
            " -4.05621768e-01 -6.84607331e-01  2.74773259e-01 -4.67370130e-01\n",
            " -3.87551687e-02  1.43994157e-01  6.29438415e-02 -3.13520018e-01\n",
            " -7.21541840e-02 -2.81511204e-01  3.77456110e-01 -4.17682379e-01\n",
            " -1.02059971e-02 -5.80804909e-01 -1.90349956e-01 -6.42319495e-03\n",
            "  5.84281978e-01 -3.74857559e-01  2.44069258e-02 -1.88913994e-01\n",
            "  1.17604047e-01 -4.30791087e-01  8.44162371e-01 -1.20077171e-01\n",
            "  5.79479032e-01  1.72858098e-01  3.41478709e-01 -2.86000921e-01\n",
            "  9.87504372e-02  3.31633431e-01 -7.69850464e-01 -6.59781544e-01\n",
            "  3.09334841e-01 -5.40607910e-01 -4.73815301e-01 -8.31923767e-03\n",
            "  1.43654382e-01  2.92456974e-01  4.08453212e-01  2.75956862e-01\n",
            " -6.58942210e-01 -2.68815041e-01  2.63432764e-01  2.56080109e-01\n",
            "  5.89997643e-01 -8.13593641e-01 -1.87669051e-01  2.23738387e-03\n",
            " -3.46486756e-01 -5.29961445e-01 -3.21595280e-01  4.80736279e-01\n",
            " -2.71715291e-01 -6.25599261e-01  4.05756268e-01 -2.70480087e-01\n",
            "  3.14404499e-01  1.02963005e-01 -1.37537800e-01  1.66136979e-01\n",
            " -1.30700558e-01 -2.61127476e-01  1.20339108e-01 -4.12606750e-01\n",
            " -2.13026497e-02 -8.45299397e-01  9.58447511e-02  2.39542237e-02\n",
            " -1.23593253e-01  8.73654948e-02 -1.29442269e-01 -7.71529863e-01\n",
            "  1.46955626e-01  1.99147069e-02 -5.74532929e-01 -5.09241114e-02\n",
            "  1.47354007e-01  4.92986474e-02 -1.34523470e-01 -7.70472573e-02\n",
            " -4.18492443e-01 -3.33047632e-01 -3.31203945e-02 -2.51631409e-01\n",
            " -1.36512102e-01  8.61047994e-01  1.94835701e-01  3.65847640e-01\n",
            "  7.15276430e-02  2.14284791e-02 -1.28334448e-02  1.74101606e-01\n",
            " -1.80450218e-02 -2.57645763e-01 -9.50678809e-01  4.91521950e-02\n",
            "  2.83560261e-01  2.44026015e-01 -2.34158006e-01  2.57326110e-01\n",
            " -6.57535019e-01  1.47348362e-01  6.21470542e-01  1.26695360e-01\n",
            "  1.76703605e-01  2.48082215e-01 -7.10551059e-01  3.35616929e-01\n",
            " -1.38701153e-01 -9.92602794e-02  1.23892116e-01  4.55836114e-02\n",
            " -1.47377821e+00 -9.12544797e-02  7.38174564e-01  3.35208674e-01\n",
            "  3.20881513e-01 -1.14204462e-01  2.81711170e-01 -2.30801542e-01\n",
            " -8.87078096e-01  5.11640406e-01  3.60382133e-02 -2.84384667e-01\n",
            "  5.97474384e-02 -1.23015631e-01 -5.83460424e-01  4.35430866e-01\n",
            " -3.60186966e-01  2.26586220e-01 -1.00712048e-01 -2.19836804e-01\n",
            " -2.24852949e-01  3.08018653e-01 -2.23203503e-01 -5.84903858e-01\n",
            " -2.60176464e-01  2.68149823e-01  1.00855399e+00 -8.48575110e-02\n",
            "  1.33895829e-01  2.52732275e-01  2.97754828e-02  6.58850727e-01\n",
            "  2.34605251e-01 -3.81226164e-01 -1.72976424e-01  5.25951956e-01\n",
            "  2.50132200e-01 -6.55924467e-01  8.16275963e-02 -3.09964842e-01\n",
            "  1.38465713e-03  4.22647194e-02 -3.05191719e-02  4.62810757e-01\n",
            " -1.59575642e-02 -5.68460590e-01 -7.80864062e-02  4.91999775e-01]\n",
            "Predicted class for each model:\n",
            "Logistic Regression: 2\n",
            "\n",
            "Explaining predictions for Logistic Regression...\n",
            "------------------------------------------------------------\n",
            "SVM: 2\n",
            "\n",
            "Explaining predictions for SVM...\n",
            "------------------------------------------------------------\n",
            "Random Forest: 2\n",
            "\n",
            "Explaining predictions for Random Forest...\n",
            "------------------------------------------------------------\n",
            "Naive Bayes: 2\n",
            "\n",
            "Explaining predictions for Naive Bayes...\n",
            "------------------------------------------------------------\n",
            "Decision Tree: 2\n",
            "\n",
            "Explaining predictions for Decision Tree...\n",
            "------------------------------------------------------------\n",
            "AdaBoost: 2\n",
            "\n",
            "Explaining predictions for AdaBoost...\n",
            "------------------------------------------------------------\n",
            "MLP: 2\n",
            "\n",
            "Explaining predictions for MLP...\n",
            "------------------------------------------------------------\n",
            "KNN: 2\n",
            "\n",
            "Explaining predictions for KNN...\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [20:42:11] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost: 2\n",
            "\n",
            "Explaining predictions for XGBoost...\n",
            "------------------------------------------------------------\n",
            "Stacking (All Models): Skipped LIME explanation (complex pipeline)\n"
          ]
        }
      ]
    }
  ]
}