{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nvn2MRZ55_h",
        "outputId": "907113b2-f837-4c78-8c99-3a3588732ccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "452\n",
            "1131\n",
            "1728\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"embedded_dataset_deberta.csv\")\n",
        "low = data[data[\"label\"] == 1]\n",
        "med = data[data[\"label\"] == 2]\n",
        "high = data[data[\"label\"] == 3]\n",
        "\n",
        "print(len(low))\n",
        "print(len(med))\n",
        "print(len(high))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "# Drop non-numeric columns (like 'Student' and 'Teacher')\n",
        "data=data.dropna(subset=['label'] + data.columns[:768].tolist())\n",
        "X = data\n",
        "\n",
        "# Convert column names to strings\n",
        "X.columns = X.columns.astype(str)\n",
        "\n",
        "# Target column\n",
        "y = data['label']\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Create DataFrame from resampled data\n",
        "X_resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "y_resampled_df = pd.DataFrame(y_resampled, columns=['Label'])\n",
        "\n",
        "# Combine into one DataFrame\n",
        "balanced_df = pd.concat([X_resampled_df, y_resampled_df], axis=1)\n",
        "\n",
        "# Save to Excel\n",
        "balanced_df.to_excel(\"final_student_embeddings_upsampled.xlsx\", index=False)\n",
        "print(\"Upsampled dataset saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XSlIkuR6vDv",
        "outputId": "b8a3b5ab-2730-4f92-b527-91ef5a7e88a0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upsampled dataset saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_excel(\"final_student_embeddings_upsampled.xlsx\")\n",
        "low = data[data[\"label\"] == 1]\n",
        "med = data[data[\"label\"] == 2]\n",
        "high = data[data[\"label\"] == 3]\n",
        "\n",
        "print(len(low))\n",
        "print(len(med))\n",
        "print(len(high))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8N4Z3vj8Cau",
        "outputId": "5f5231f8-d769-433e-85cd-b2c4c4e810ed"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1728\n",
            "1728\n",
            "1728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3R4vclqW9Dov",
        "outputId": "8661d5dc-2fc0-4ffd-a185-6862986257a2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/275.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from lime) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from lime) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lime) (1.15.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from lime) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.11/dist-packages (from lime) (1.6.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.11/dist-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (11.2.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2025.3.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=3891d266115fb52b2bb4757074c62b458e095bec4b823fc1ffd2892e71794312\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/fa/a3/9c2d44c9f3cd77cf4e533b58900b2bf4487f2a17e8ec212a3d\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NeighborhoodComponentsAnalysis\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Optional: XGBoost\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    xgb_available = True\n",
        "except ImportError:\n",
        "    xgb_available = False\n",
        "\n",
        "# Load updated dataset\n",
        "df = pd.read_excel(\"final_student_embeddings_upsampled.xlsx\")\n",
        "\n",
        "# Update this column name if needed\n",
        "target_column = \"label\"\n",
        "\n",
        "# Features and Labels\n",
        "X = df.iloc[:, :768].values\n",
        "y = df[target_column]\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# NCA\n",
        "nca = NeighborhoodComponentsAnalysis(n_components=2, random_state=42)\n",
        "X_nca = nca.fit_transform(X, y_encoded)\n",
        "\n",
        "# Analyze top contributing original features to NCA components\n",
        "nca_components = nca.components_  # Shape: (2, 768)\n",
        "top_k = 10  # Top N contributing features to show\n",
        "\n",
        "for i in range(2):\n",
        "    component = nca_components[i]\n",
        "    top_indices = np.argsort(np.abs(component))[::-1][:top_k]\n",
        "    print(f\"\\nüîç Top {top_k} original features contributing to NCA Component {i + 1}:\")\n",
        "    for idx in top_indices:\n",
        "        print(f\"Feature {idx}: Weight = {component[idx]:.4f}\")\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_nca)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grids for hyperparameter tuning\n",
        "param_grids = {\n",
        "    \"Logistic Regression\": {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'solver': ['lbfgs', 'liblinear']\n",
        "    },\n",
        "    \"SVM\": {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['linear', 'rbf']\n",
        "    },\n",
        "    \"Random Forest\": {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [None, 5, 10],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 5]\n",
        "    },\n",
        "    \"Naive Bayes\": {},\n",
        "    \"Decision Tree\": {\n",
        "        'max_depth': [None, 5, 10],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 5]\n",
        "    },\n",
        "    \"AdaBoost\": {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 1.0]\n",
        "    },\n",
        "    \"MLP\": {\n",
        "        'hidden_layer_sizes': [(50,), (100,), (100, 50)],\n",
        "        'activation': ['relu', 'tanh'],\n",
        "        'learning_rate': ['constant', 'adaptive']\n",
        "    },\n",
        "    \"KNN\": {\n",
        "        'n_neighbors': list(range(1, 21)),\n",
        "        'weights': ['uniform', 'distance']\n",
        "    }\n",
        "}\n",
        "\n",
        "if xgb_available:\n",
        "    param_grids[\"XGBoost\"] = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'learning_rate': [0.01, 0.1, 0.3]\n",
        "    }\n",
        "\n",
        "# Base models\n",
        "base_models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
        "    \"SVM\": SVC(probability=True, random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
        "    \"MLP\": MLPClassifier(max_iter=500, random_state=42),\n",
        "    \"KNN\": KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "if xgb_available:\n",
        "    base_models[\"XGBoost\"] = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "\n",
        "# Hyperparameter tuning\n",
        "best_models = {}\n",
        "for name, model in base_models.items():\n",
        "    if param_grids[name]:\n",
        "        print(f\"Tuning {name}...\")\n",
        "        grid_search = GridSearchCV(\n",
        "            estimator=model,\n",
        "            param_grid=param_grids[name],\n",
        "            cv=5,\n",
        "            scoring='accuracy',\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        best_models[name] = grid_search.best_estimator_\n",
        "        print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "        best_models[name] = model\n",
        "        print(f\"No tuning required for {name}\")\n",
        "\n",
        "# Stacking classifier\n",
        "stacking_estimators = [(name.lower().replace(\" \", \"_\"), model) for name, model in best_models.items()]\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=stacking_estimators,\n",
        "    final_estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Add stacking\n",
        "models = best_models.copy()\n",
        "models[\"Stacking (All Models)\"] = stacking_clf\n",
        "\n",
        "# Evaluate\n",
        "for name, model in models.items():\n",
        "    print(f\" {name}\")\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    train_acc = accuracy_score(y_train, model.predict(X_train))\n",
        "    test_acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Training Accuracy: {train_acc:.4f}\")\n",
        "    print(f\"Testing Accuracy:  {test_acc:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    target_names = [str(label) for label in label_encoder.classes_]\n",
        "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import matplotlib.pyplot as plt  # <-- Add this import\n",
        "\n",
        "# Prepare the test set in original feature space\n",
        "_, X_test_original, _, y_test_original = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42\n",
        ")\n",
        "instance_original = X_test_original[0]\n",
        "\n",
        "# Set up LIME explainer for original features\n",
        "explainer = LimeTabularExplainer(\n",
        "    training_data=X,\n",
        "    feature_names=[f\"emb_{i}\" for i in range(X.shape[1])],\n",
        "    class_names=[str(label) for label in label_encoder.classes_],\n",
        "    mode='classification',\n",
        "    random_state=42,\n",
        "    discretize_continuous=False\n",
        ")\n",
        "\n",
        "print(\"\\nLIME Explanations for Test Instance (First Sample, original features):\")\n",
        "print(f\"Instance values: {instance_original}\")\n",
        "print(\"Predicted class for each model:\")\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Skip stacking for LIME unless you want advanced handling\n",
        "    if name == \"Stacking (All Models)\":\n",
        "        print(f\"{name}: Skipped LIME explanation (complex pipeline)\")\n",
        "        continue\n",
        "\n",
        "    # Build pipeline: StandardScaler -> NCA -> Model (for LIME only)\n",
        "    lime_pipeline = make_pipeline(\n",
        "        StandardScaler(),\n",
        "        NeighborhoodComponentsAnalysis(n_components=2, random_state=42),\n",
        "        model\n",
        "    )\n",
        "    # Fit pipeline on ALL original data (NCA is supervised)\n",
        "    lime_pipeline.fit(X, y_encoded)\n",
        "\n",
        "    pred = lime_pipeline.predict([instance_original])[0]\n",
        "    print(f\"{name}: {label_encoder.inverse_transform([pred])[0]}\")\n",
        "\n",
        "    print(f\"\\nExplaining predictions for {name}...\")\n",
        "    try:\n",
        "        explanation = explainer.explain_instance(\n",
        "            data_row=instance_original,\n",
        "            predict_fn=lime_pipeline.predict_proba,\n",
        "            num_features=10,  # Show top 10 original features\n",
        "            num_samples=5000\n",
        "        )\n",
        "        # Save as HTML\n",
        "        explanation.save_to_file(f\"lime_explanation_{name.replace(' ', '_')}_origfeatures.html\")\n",
        "        # Save as image\n",
        "        fig = explanation.as_pyplot_figure()\n",
        "        plt.tight_layout()\n",
        "        fig.savefig(f\"lime_explanation_{name.replace(' ', '_')}_origfeatures.png\")\n",
        "        plt.close(fig)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to generate explanation for {name}: {str(e)}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6ZiB2wK80jY",
        "outputId": "710fdb25-f888-4f8e-f11a-74273ba58cc8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Top 10 original features contributing to NCA Component 1:\n",
            "Feature 637: Weight = -83.3366\n",
            "Feature 219: Weight = -72.4171\n",
            "Feature 730: Weight = -70.8418\n",
            "Feature 383: Weight = -70.7549\n",
            "Feature 326: Weight = 68.2803\n",
            "Feature 343: Weight = -66.9609\n",
            "Feature 582: Weight = -61.8442\n",
            "Feature 305: Weight = 60.6495\n",
            "Feature 448: Weight = 59.7786\n",
            "Feature 220: Weight = -59.1081\n",
            "\n",
            "üîç Top 10 original features contributing to NCA Component 2:\n",
            "Feature 315: Weight = 67.7838\n",
            "Feature 650: Weight = 67.3828\n",
            "Feature 282: Weight = 65.3098\n",
            "Feature 102: Weight = -61.5720\n",
            "Feature 195: Weight = 58.1011\n",
            "Feature 558: Weight = 58.0874\n",
            "Feature 331: Weight = 56.4816\n",
            "Feature 184: Weight = 56.4578\n",
            "Feature 222: Weight = 56.4399\n",
            "Feature 84: Weight = 55.9850\n",
            "Tuning Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'C': 0.01, 'solver': 'lbfgs'}\n",
            "Tuning SVM...\n",
            "Best parameters for SVM: {'C': 1, 'kernel': 'rbf'}\n",
            "Tuning Random Forest...\n",
            "Best parameters for Random Forest: {'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
            "No tuning required for Naive Bayes\n",
            "Tuning Decision Tree...\n",
            "Best parameters for Decision Tree: {'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
            "Tuning AdaBoost...\n",
            "Best parameters for AdaBoost: {'learning_rate': 1.0, 'n_estimators': 200}\n",
            "Tuning MLP...\n",
            "Best parameters for MLP: {'activation': 'tanh', 'hidden_layer_sizes': (100, 50), 'learning_rate': 'constant'}\n",
            "Tuning KNN...\n",
            "Best parameters for KNN: {'n_neighbors': 14, 'weights': 'uniform'}\n",
            "Tuning XGBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [19:50:42] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50}\n",
            "üìå Logistic Regression\n",
            "Training Accuracy: 0.8073\n",
            "Testing Accuracy:  0.8120\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.87      0.95      0.91       331\n",
            "           2       0.79      0.77      0.78       344\n",
            "           3       0.77      0.73      0.75       362\n",
            "\n",
            "    accuracy                           0.81      1037\n",
            "   macro avg       0.81      0.82      0.81      1037\n",
            "weighted avg       0.81      0.81      0.81      1037\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå SVM\n",
            "Training Accuracy: 0.8179\n",
            "Testing Accuracy:  0.8303\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.91      0.94      0.93       331\n",
            "           2       0.77      0.82      0.80       344\n",
            "           3       0.81      0.74      0.77       362\n",
            "\n",
            "    accuracy                           0.83      1037\n",
            "   macro avg       0.83      0.83      0.83      1037\n",
            "weighted avg       0.83      0.83      0.83      1037\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå Random Forest\n",
            "Training Accuracy: 0.8247\n",
            "Testing Accuracy:  0.8245\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.92      0.92      0.92       331\n",
            "           2       0.77      0.81      0.79       344\n",
            "           3       0.79      0.75      0.77       362\n",
            "\n",
            "    accuracy                           0.82      1037\n",
            "   macro avg       0.83      0.83      0.83      1037\n",
            "weighted avg       0.82      0.82      0.82      1037\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå Naive Bayes\n",
            "Training Accuracy: 0.8073\n",
            "Testing Accuracy:  0.8226\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.86      0.97      0.91       331\n",
            "           2       0.78      0.81      0.80       344\n",
            "           3       0.83      0.70      0.76       362\n",
            "\n",
            "    accuracy                           0.82      1037\n",
            "   macro avg       0.82      0.83      0.82      1037\n",
            "weighted avg       0.82      0.82      0.82      1037\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå Decision Tree\n",
            "Training Accuracy: 0.8269\n",
            "Testing Accuracy:  0.8284\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.91      0.94      0.93       331\n",
            "           2       0.77      0.82      0.80       344\n",
            "           3       0.80      0.73      0.77       362\n",
            "\n",
            "    accuracy                           0.83      1037\n",
            "   macro avg       0.83      0.83      0.83      1037\n",
            "weighted avg       0.83      0.83      0.83      1037\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå AdaBoost\n",
            "Training Accuracy: 0.7892\n",
            "Testing Accuracy:  0.7907\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.79      0.95      0.87       331\n",
            "           2       0.81      0.68      0.74       344\n",
            "           3       0.77      0.74      0.76       362\n",
            "\n",
            "    accuracy                           0.79      1037\n",
            "   macro avg       0.79      0.79      0.79      1037\n",
            "weighted avg       0.79      0.79      0.79      1037\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå MLP\n",
            "Training Accuracy: 0.8177\n",
            "Testing Accuracy:  0.8284\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.92      0.94      0.93       331\n",
            "           2       0.78      0.81      0.79       344\n",
            "           3       0.80      0.75      0.77       362\n",
            "\n",
            "    accuracy                           0.83      1037\n",
            "   macro avg       0.83      0.83      0.83      1037\n",
            "weighted avg       0.83      0.83      0.83      1037\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå KNN\n",
            "Training Accuracy: 0.8240\n",
            "Testing Accuracy:  0.8293\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.90      0.94      0.92       331\n",
            "           2       0.77      0.83      0.80       344\n",
            "           3       0.81      0.73      0.77       362\n",
            "\n",
            "    accuracy                           0.83      1037\n",
            "   macro avg       0.83      0.83      0.83      1037\n",
            "weighted avg       0.83      0.83      0.83      1037\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå XGBoost\n",
            "Training Accuracy: 0.8360\n",
            "Testing Accuracy:  0.8293\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.92      0.94      0.93       331\n",
            "           2       0.77      0.82      0.79       344\n",
            "           3       0.80      0.74      0.77       362\n",
            "\n",
            "    accuracy                           0.83      1037\n",
            "   macro avg       0.83      0.83      0.83      1037\n",
            "weighted avg       0.83      0.83      0.83      1037\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìå Stacking (All Models)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [19:51:02] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [19:51:20] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [19:52:36] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.8242\n",
            "Testing Accuracy:  0.8293\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.91      0.94      0.92       331\n",
            "           2       0.78      0.81      0.80       344\n",
            "           3       0.80      0.75      0.77       362\n",
            "\n",
            "    accuracy                           0.83      1037\n",
            "   macro avg       0.83      0.83      0.83      1037\n",
            "weighted avg       0.83      0.83      0.83      1037\n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "LIME Explanations for Test Instance (First Sample, original features):\n",
            "Instance values: [ 4.92178535e-02 -3.63469691e-02 -8.16499473e-02 -3.15967160e-02\n",
            "  2.86152071e-02  6.95988795e-02 -1.07600847e-01 -5.01220247e-03\n",
            "  1.25207436e-01 -2.84034600e-02 -8.35321420e-02  4.19806348e-02\n",
            " -1.69021700e-02 -2.08184492e-02 -1.58845358e-02 -1.82584623e-02\n",
            "  3.24840772e-02  2.05828715e-02  5.59502455e-02  1.61553675e-04\n",
            "  6.90197896e-02  7.57461447e-02  2.11278543e-02 -5.26399194e-02\n",
            "  6.67873403e-02  8.28465722e-03  3.39609379e-02  8.49798125e-02\n",
            "  2.70814480e-03  1.12986842e-03 -4.00277684e-02  4.63448331e-02\n",
            "  1.19862109e-02 -2.31812870e-02  9.94523229e-02  4.19530379e-02\n",
            "  3.51627338e-02 -2.05440474e-02  5.79107352e-02  8.20034840e-02\n",
            "  7.26184474e-04 -1.08458360e-01 -5.00791801e-02  8.21357169e-02\n",
            " -1.98985449e-02  8.15004208e-02  2.54213112e-02  2.50958228e-02\n",
            "  7.94624106e-02  1.56043491e-02  1.33584310e-02  9.11630847e-04\n",
            "  5.35901892e-04  3.82085892e-03  3.92300926e-02  9.43644918e-02\n",
            " -1.62464274e-02 -1.06981594e-02  5.18025037e-02  4.55447860e-02\n",
            " -6.36417771e-02  6.65245527e-02 -7.04748133e-02  9.39940908e-03\n",
            " -7.01339147e-02  3.68171708e-02 -4.39264085e-03 -4.60478579e-02\n",
            "  3.51235034e-03  6.59030899e-02 -3.12787443e-03 -4.32481135e-02\n",
            "  7.31496217e-02 -1.02355328e-01 -9.65519666e-04  6.09110708e-02\n",
            "  3.62193611e-03 -1.23213937e-02 -8.81569916e-02 -6.59946973e-02\n",
            " -1.21241117e-02  3.56482917e-02 -6.73530277e-02 -1.65158825e-02\n",
            " -8.22591705e-02 -2.62733538e-02  7.13578341e-02 -9.02770820e-03\n",
            "  5.36504194e-02 -7.16442736e-02  4.59514993e-02 -1.20744713e-02\n",
            " -2.65477881e-03 -6.32685511e-03 -2.78357552e-02 -3.38986545e-02\n",
            "  2.14746924e-02 -9.96594628e-01  2.23881100e-02 -3.22566826e-02\n",
            " -4.05580062e-02 -1.06703103e-01 -2.60495283e-02  7.65997871e-02\n",
            " -1.52009807e-01  5.99611936e-02  9.31680209e-03  3.91082318e-02\n",
            "  7.67147716e-02 -6.23059146e-02  7.04057459e-02 -3.30362133e-02\n",
            "  3.24862336e-02  1.39355012e-02 -4.01307345e-02 -7.76900434e-02\n",
            " -6.77685627e-02  9.14405152e-02  1.47040930e-02  3.34549786e-02\n",
            "  6.45721729e-03  5.76895685e-02  2.10636721e-02 -1.43306830e-02\n",
            "  4.46992192e-02  1.83005240e-02  2.63157881e-02  1.36649145e-02\n",
            " -1.16882017e-02  5.78824701e-02 -6.69895716e-03  3.23817379e-02\n",
            " -1.92235540e-02 -2.01947760e-02 -3.34448462e-02  7.25223451e-02\n",
            " -2.12171957e-02 -4.34769750e-02 -3.10691321e-01 -9.91805246e-03\n",
            " -1.60862393e-02  3.63249680e-02 -4.21881697e-02  3.07899003e-02\n",
            "  6.91379513e-02 -5.13074653e-03  8.10735361e-02  3.75805077e-02\n",
            " -1.07326143e-02 -8.78505231e-03  3.66892770e-02 -8.40177530e-02\n",
            "  3.83114706e-02  3.75113348e-02 -2.04965922e-03  6.42578636e-02\n",
            " -3.92057347e-02  8.32798417e-02  8.66001060e-04 -1.36722886e-01\n",
            "  4.39221044e-02  4.95613565e-02  7.43586206e-03 -9.61588254e-03\n",
            " -1.26383260e-02  2.44840968e-02  2.02318904e-02 -1.05348061e-02\n",
            " -2.38170136e-02 -4.32807098e-02  2.84798141e-02 -4.18738598e-03\n",
            "  8.23043504e-02  8.29727342e-02 -4.51020961e-02 -9.11556109e-02\n",
            "  2.90322786e-02 -1.51919429e-02  3.52938143e-02  1.04422086e-02\n",
            "  6.53653849e-03 -9.67008647e-03 -5.31699164e-02  8.51339182e-03\n",
            "  1.79702003e-02  5.34046920e-02  4.04182611e-02  4.42890118e-02\n",
            " -7.26915235e-02  3.42733905e-02  5.32803708e-02  8.06021211e-03\n",
            " -1.25753939e-01  2.81862483e-02  4.83685410e-02  5.84128674e-02\n",
            " -2.77396236e-02 -5.88821406e-02  3.52734001e-02  2.99124196e-02\n",
            "  5.82106878e-02 -8.40988722e-02  3.27975840e-02 -2.93792095e-02\n",
            "  2.75753646e-02  4.88956642e-02 -2.43039939e-02 -4.71380377e-03\n",
            " -3.78270167e-03  8.89674676e-04 -3.97485451e-02 -3.66388246e-03\n",
            " -4.87074334e-02 -4.08735043e-02  1.84268847e-02 -6.94341194e-02\n",
            " -6.68855235e-02 -4.01829808e-02  1.41755512e-02 -1.00832790e-02\n",
            "  3.58661459e-02  6.99036024e-02 -5.75772738e-02 -7.58457922e-04\n",
            " -1.26812895e-02 -2.66599468e-02 -1.88508957e-02 -4.55176759e-02\n",
            "  9.50513369e-02  3.32318800e-02 -9.33964657e-03  2.96186160e-02\n",
            "  5.58173454e-02  3.48424553e-02 -1.22838084e-02  2.92912829e-02\n",
            "  3.13113987e-02 -3.90639627e-02 -6.11250682e-02  3.66379217e-02\n",
            " -1.79009775e-02  2.10611391e-02 -5.22561146e-02 -8.40561628e-03\n",
            " -4.18431719e-02 -9.09151852e-03  6.17837652e-02  5.44729748e-02\n",
            " -8.25531937e-02  5.61851136e-03 -3.68168782e-02  7.46506290e-03\n",
            " -1.89812062e-02 -5.69922100e-03 -5.69588145e-03  1.28137612e-01\n",
            "  4.37163979e-02 -4.93490779e-02  1.47036964e-01 -7.35384717e-02\n",
            "  6.76799179e-02  3.54435021e-02 -1.00774272e-02 -5.95876701e-02\n",
            "  4.56647068e-03  2.66338373e-02  8.64793174e-02  2.55024123e-02\n",
            "  2.98099435e-02  5.11031368e-02 -5.53447056e-03 -6.39600608e-03\n",
            "  1.62566457e-02  1.57314289e-02 -5.50994888e-02 -1.68621737e-02\n",
            " -5.69456780e-02  4.39446826e-02  1.04108380e-02  1.00815313e-02\n",
            "  4.24791873e-03  1.73681841e-02  7.60209307e+00 -6.84499240e-03\n",
            "  1.15889153e-02  9.93752334e-02  1.41035523e-02  1.30251351e-01\n",
            "  2.38166790e-02  2.94494725e-02  5.22118693e-02 -4.25444192e-02\n",
            " -4.28578016e-02  1.04409422e-02  4.73263181e-02  3.61168539e-02\n",
            "  4.88140511e-03 -1.43472975e-02  1.05117200e-02 -7.90968446e-02\n",
            " -2.03390979e-02  5.77366142e-02 -7.30910165e-02 -8.96481443e-02\n",
            "  2.14216181e-02 -4.87107555e-03  5.05373025e-02 -3.21617029e-02\n",
            " -8.90665314e-02  4.63553946e-02  3.82351591e-03 -1.34014454e-02\n",
            " -1.16900200e-02  4.94554626e-02 -4.52058181e-02 -1.02004022e-01\n",
            "  7.46481147e-03  1.25157119e-02 -3.33547999e-02 -9.39267271e-03\n",
            "  4.69717417e-02 -3.96734579e-03  1.62666122e-01  5.59089663e-02\n",
            "  6.27737697e-02 -5.12120946e-02  7.69096618e-02 -8.20767905e-02\n",
            "  9.82502788e-02 -2.83296869e-04 -7.99255367e-02 -5.12707364e-03\n",
            "  3.60555763e-02 -3.44920863e-03 -4.10545916e-02 -5.50780992e-02\n",
            " -1.92511430e-02  6.24867850e-02  4.71903934e-02  1.02029222e-02\n",
            " -1.07479590e-02 -8.04157833e-02  3.35833822e-02  4.57107841e-02\n",
            " -3.80740508e-02 -3.60949290e-02 -9.02724690e-02  3.18632799e-02\n",
            "  3.29457105e-02 -9.67934174e-03 -5.17734100e-02  3.94085704e-02\n",
            "  6.05823262e-02 -1.01845772e-01  3.38154261e-02  7.90489996e-02\n",
            " -5.66780640e-02 -3.63510561e-02 -2.23083029e-02  3.64608734e-03\n",
            " -5.40913506e-03  6.69231335e-02 -4.10369152e-05  1.69230233e-02\n",
            " -7.78834035e-02  5.10290493e-02 -1.74062018e-02  1.98798564e-02\n",
            "  3.64239802e-02  3.28893878e-02 -1.07808364e-02  4.29838757e-02\n",
            "  3.39699219e-02  1.33576682e-01 -2.23544923e-02 -4.21238626e-02\n",
            " -6.18014687e-03 -3.14955282e-02 -5.23788263e-02 -1.89458576e-02\n",
            " -2.21653782e-02  9.92302813e-03  4.91441986e-02  7.46569592e-02\n",
            "  2.45462723e-02  5.57513492e-02  1.21289418e-01  7.18329701e-02\n",
            "  9.11788701e-03  2.51436835e-02  3.48424090e-02  7.74533698e-02\n",
            " -2.92267921e-02 -4.76031324e-02  9.80560606e-03  2.50474020e-02\n",
            "  1.71945143e-01 -1.73676828e-02 -9.23897081e-02  2.81475559e-02\n",
            "  8.00877779e-02  5.59343523e-02  5.84945989e-02  2.40916947e-02\n",
            " -8.39836011e-02  1.27708593e-02 -7.57495850e-02 -2.76090260e-02\n",
            " -1.20400269e-01  4.02378062e-02 -4.60289565e-03 -1.23534999e-01\n",
            "  1.01345422e-02  1.37412315e-03 -4.53294563e-02 -5.70244811e-03\n",
            " -2.71896961e-02  8.54097596e-02  4.93031118e-02 -6.91055940e-03\n",
            " -4.16021613e-02  3.71294264e-02 -1.79565877e-02 -3.59043845e-02\n",
            " -5.33889001e-02  4.32004203e-02 -3.84056604e-02  9.47828922e-02\n",
            " -6.28838713e-02 -3.24114111e-03  2.23883931e-03  4.51596659e-02\n",
            "  9.59184833e-03 -7.91869599e-02  2.34569073e-03  5.85755476e-02\n",
            " -4.54361752e-02 -2.94658679e-03  9.71364768e-03  7.69977555e-02\n",
            " -5.89390478e-02  8.19633251e-02  8.42351385e-02  6.37666526e-02\n",
            "  2.44679255e-02  8.15760405e-02  8.63897484e-02 -3.22945998e-03\n",
            "  8.58630913e-03  3.89945641e-02  5.21480102e-02 -6.80941432e-03\n",
            "  4.45844167e-02 -6.79342492e-02 -3.74977359e-02  9.65476864e-03\n",
            " -4.15497130e-02  8.60636086e-03  2.19653584e-02  8.56050499e-03\n",
            "  5.44181754e-02 -9.10415199e-03 -1.81576552e-02  7.79683820e-02\n",
            "  1.53734069e-02 -7.54091109e-02 -3.60979270e-02  2.98079406e-02\n",
            " -4.16453499e-03 -2.32596287e-02 -2.75054189e-02  2.80961187e-02\n",
            " -2.06561675e-02  6.72477928e-02  3.87781738e-02  3.83881433e-02\n",
            " -8.31848108e-03  6.50154804e-02 -1.06479248e-01  1.74306927e-03\n",
            " -4.82951883e-02  1.11378054e-01  6.75557472e-02 -7.67193503e-02\n",
            "  4.65073591e-02  5.06517066e-02 -1.10230326e-02  2.45966321e-02\n",
            "  7.80097436e-02  1.47709732e-02  4.57136880e-02  6.63875843e-02\n",
            "  1.82250857e-03 -7.46968648e-02  7.62284136e-02  9.52545514e-03\n",
            " -9.37924520e-03  5.19555466e-02  7.00941474e-02 -1.06333954e-01\n",
            " -5.31770107e-02 -1.64382968e-02 -5.16096578e-02 -7.34878190e-03\n",
            "  3.68820591e-02 -5.16383768e-02 -2.77824488e-02 -7.67658613e-02\n",
            "  8.31990781e-02 -1.99444831e-02  7.56195525e-02 -4.45731320e-02\n",
            "  3.14173262e-02  7.83224882e-02 -2.95432806e-02  3.30931635e-02\n",
            " -5.31618773e-03 -7.84998569e-02 -3.97399791e-02 -5.47469088e-02\n",
            "  7.94952478e-02 -2.62084144e-02  5.45257168e-02  2.62833134e-02\n",
            "  5.18746405e-02 -1.77843891e-02  9.54379267e-02 -6.49996943e-03\n",
            "  9.79397543e-02  5.03167186e-02 -6.47552977e-02 -2.50931560e-03\n",
            " -4.76445086e-02  8.83598130e-02 -7.38097074e-02  2.51546572e-02\n",
            "  9.97712611e-02  2.54270091e-02 -2.22512638e-02 -1.48358196e-02\n",
            "  5.06673094e-02 -4.12596354e-02 -4.59819562e-03  9.06329946e+00\n",
            "  1.89638258e-02 -2.19799816e-02  4.70387564e-02  4.17736379e-02\n",
            " -7.15443223e-02  1.41873470e-02 -3.06560565e-02 -7.71208599e-02\n",
            " -8.28384651e-03  8.56038214e-02  4.45040022e-02  3.82855077e-02\n",
            " -1.58661457e-02 -2.62510045e-02  2.02044659e-02 -5.97249218e-03\n",
            " -6.56361493e-02 -6.86329838e-02  6.35095195e-02 -5.44193007e-02\n",
            "  7.86163552e-03  8.67559350e-02 -2.91128931e-02 -4.36152993e-03\n",
            " -2.72170667e-02 -2.13624945e-02  1.32561741e-02  1.90430947e-02\n",
            "  8.54847935e-03 -4.84190451e-02  3.87623334e-03 -4.78709325e-03\n",
            "  6.27057520e-02 -8.02201292e-02 -8.03296962e-02 -4.25989963e-02\n",
            " -9.66919981e-02  7.06877523e-02  9.25457517e-03 -3.46202435e-02\n",
            " -2.41377483e-02 -5.41919639e-03 -3.55612710e-02  5.50723364e-02\n",
            "  3.87342465e-02 -2.47359153e-02  1.88014339e-02  4.02642553e-03\n",
            " -8.06635729e-04  1.66653081e+00 -1.17201854e-02 -5.37916406e-02\n",
            "  3.02451734e-03  3.51703028e-02  2.67473931e-02  2.34357868e-02\n",
            " -1.49932058e-02  7.83054071e-02 -1.18330351e-01 -3.81788869e-03\n",
            "  8.58661678e-02 -2.57676291e-02  4.50603671e-02 -5.26733205e-02\n",
            "  1.36087333e-03  6.95000162e-02  2.31973061e-02 -3.74621934e-02\n",
            " -5.90238613e-04 -2.24347983e-02 -2.81053032e-02  4.68870739e-02\n",
            " -6.81714385e-02  2.49658346e-02 -6.38442675e-02 -1.15517143e-01\n",
            " -5.71924659e-02 -3.33253107e-02 -4.32017222e-03 -7.19511898e-05\n",
            " -1.05472816e-02  8.99954843e-02 -5.10902058e-04 -3.91387932e-02\n",
            " -7.96495133e-02 -1.04250338e-01  3.31699048e-02  7.79532057e-02\n",
            " -1.85761420e-02 -2.01253555e-02 -4.08243254e-03 -1.49240880e-03\n",
            "  6.16536135e-02  2.99640094e-02  2.44471787e-02 -3.62892577e-04\n",
            " -4.40728660e-02 -3.58384164e-02  8.09011161e-03 -3.04972053e-02\n",
            "  4.85039394e-02  2.22990293e-02  3.40544969e-02  8.28130841e-05\n",
            " -3.93932901e-02  3.42612690e-02 -4.91663406e-01  3.82043811e-02\n",
            " -5.59364185e-02 -6.92955431e-02 -1.33443532e-02  1.81457388e-02\n",
            "  8.11735211e-02  5.94612587e-03 -6.14979750e-03  3.84601272e-02\n",
            "  3.43783744e-03 -1.91314635e-02  1.83512501e-02  6.21485967e-02\n",
            " -2.91857467e-02 -2.52268267e-02 -1.15408312e-02 -1.17935200e-01\n",
            " -1.93022536e-02 -5.77916960e-02  2.04392090e-02  9.41275332e-03\n",
            " -9.56784234e-02  4.76957614e-02 -2.61616999e-02  1.21897311e-02\n",
            " -2.31912761e-02 -5.63173657e-02 -1.13612652e-02  1.41768645e-02\n",
            "  2.83865944e-02  1.27819583e-02 -1.59091934e-02 -9.14667527e-02\n",
            "  3.13319222e-02  3.14734079e-02 -2.96365545e-02 -1.22595925e-01\n",
            " -1.15158681e-02  5.03618306e-03  6.44513782e-02  8.59237015e-02\n",
            " -2.28149580e-02  2.96381551e-02 -8.82303278e-04 -4.09365556e-02\n",
            " -1.74062069e-02 -7.43679467e-02  3.55155358e-02  2.09356487e-02\n",
            " -1.06584904e-01 -9.73505560e-02  3.80307393e-02 -4.02333790e-03\n",
            "  5.04847070e-02  1.17328496e-02 -4.32064269e-02  2.02295297e-02\n",
            " -3.25885341e-02  4.60612716e-02 -1.20094646e-01 -4.40880245e-02\n",
            " -4.37681323e-03  2.94141584e-02  5.16976968e-02 -9.55586825e-02\n",
            "  6.37778989e-02  1.07551023e-01  2.58596581e-02 -4.69283978e-02\n",
            "  1.38006740e-02 -3.10115353e-02  6.41844064e-02  6.80909716e-02\n",
            "  2.38878921e-02 -3.65915196e-02  3.21944691e-01  1.87843601e-02\n",
            " -6.09116073e-02  1.67069408e-03  1.70351557e-02 -2.69583229e-02\n",
            " -5.26784031e-02  6.06001339e-02 -7.15407036e-02 -1.17264677e-01\n",
            " -3.37987520e-02 -7.72643801e-03  6.84071836e-02  1.41530867e-02\n",
            " -5.20180949e-02 -2.14467462e-03 -1.09291147e-01  3.55380471e-02\n",
            " -2.90258189e-02  3.30799166e-02  1.81923619e-02 -2.58671042e-02\n",
            "  2.31222277e-02 -2.10956759e-02 -9.44591323e-02  3.42525795e-02\n",
            " -2.15641265e-02  4.63192188e+00 -8.95357931e+00  2.62686702e-02\n",
            " -4.75390322e-02 -1.68627183e-02 -1.87655714e-02 -5.25888013e-02\n",
            "  4.68834984e-02 -9.28814347e-03  3.89148451e-03  6.29516382e-02\n",
            " -2.54685114e-02  6.10160104e-03  7.08437374e-02 -5.95930237e-02]\n",
            "Predicted class for each model:\n",
            "Logistic Regression: 2\n",
            "\n",
            "Explaining predictions for Logistic Regression...\n",
            "------------------------------------------------------------\n",
            "SVM: 2\n",
            "\n",
            "Explaining predictions for SVM...\n",
            "------------------------------------------------------------\n",
            "Random Forest: 2\n",
            "\n",
            "Explaining predictions for Random Forest...\n",
            "------------------------------------------------------------\n",
            "Naive Bayes: 2\n",
            "\n",
            "Explaining predictions for Naive Bayes...\n",
            "------------------------------------------------------------\n",
            "Decision Tree: 2\n",
            "\n",
            "Explaining predictions for Decision Tree...\n",
            "------------------------------------------------------------\n",
            "AdaBoost: 1\n",
            "\n",
            "Explaining predictions for AdaBoost...\n",
            "------------------------------------------------------------\n",
            "MLP: 2\n",
            "\n",
            "Explaining predictions for MLP...\n",
            "------------------------------------------------------------\n",
            "KNN: 2\n",
            "\n",
            "Explaining predictions for KNN...\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [20:06:17] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost: 2\n",
            "\n",
            "Explaining predictions for XGBoost...\n",
            "------------------------------------------------------------\n",
            "Stacking (All Models): Skipped LIME explanation (complex pipeline)\n"
          ]
        }
      ]
    }
  ]
}